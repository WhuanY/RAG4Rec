{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    ".inter\n",
    "(g, j, s)\n",
    "g.info <- .udoc\n",
    "j.info <- .idoc\n",
    "\n",
    "g.strRep -> C \n",
    "            o          M\n",
    "            n --BGE--> L ---> out\n",
    "            c --BGE--> P ---> put\n",
    "            a          s\n",
    "j.strRep -> t\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-06 23:56:37,853 - INFO - Usable GPU: 10\n",
      "2025-01-06 23:56:46,284 - INFO - Dataset train Loaded. Shape: (512, 7)\n",
      "2025-01-06 23:56:47,449 - INFO - Dataset valid Loaded. Shape: (64, 7)\n",
      "2025-01-06 23:56:48,619 - INFO - Dataset test Loaded. Shape: (639, 7)\n"
     ]
    }
   ],
   "source": [
    "bge_path = \"/media/wuyuhuan/bge-small-zh\"\n",
    "from torch.utils.data import IterableDataset\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.clip_grad import clip_grad_norm_\n",
    "\n",
    "import logging\n",
    "\n",
    "import time\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "logging.info(f\"Usable GPU: {torch.cuda.device_count()}\")  \n",
    "tokenizer = AutoTokenizer.from_pretrained(bge_path)\n",
    "\n",
    "def same_seed(seed):\n",
    "    '''Fixes random number generator seeds for reproducibility.'''\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def dict2device(data, device):\n",
    "    for k, v in data.items():\n",
    "        if isinstance(v, dict):\n",
    "            data[k] = dict2device(v, device)\n",
    "        elif isinstance(v, torch.Tensor):\n",
    "            data[k] = v.to(device)\n",
    "    return data\n",
    "\n",
    "same_seed(42)\n",
    "\n",
    "class BGE_FTDataset(IterableDataset):\n",
    "    def __init__(self, mode: str, in_file: str, tokenizer: AutoTokenizer, ratio: float = 1.0):\n",
    "        \"\"\"\n",
    "        mode: str, one of ['train', 'valid', 'test']\n",
    "        in_file: str, path to the input csv file.\n",
    "        tokenizer: AutoTokenizer, tokenizer for the model.\n",
    "        ratio: float, the ratio of the data to be used. Default: 1.0. \n",
    "            set to 0.01 for functionality testing.\n",
    "        \"\"\"\n",
    "        self.mode = mode\n",
    "        self.dataset = pd.read_csv(in_file).sample(frac=ratio)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(bge_path)\n",
    "        logging.info(f\"Dataset {mode} Loaded. Shape: {self.dataset.shape}\")\n",
    "\n",
    "    def __iter__(self):\n",
    "        for _, row in self.dataset.iterrows():\n",
    "            cv = row['cv']\n",
    "            jd = row['jd']\n",
    "\n",
    "            tokenized_cv = self.tokenizer(\n",
    "                cv, \n",
    "                return_tensors='pt', \n",
    "                padding='max_length', \n",
    "                truncation=True, \n",
    "                max_length=512\n",
    "            )\n",
    "            \n",
    "            tokenized_jd = self.tokenizer(\n",
    "                jd, \n",
    "                return_tensors='pt', \n",
    "                padding='max_length', \n",
    "                truncation=True, \n",
    "                max_length=512\n",
    "            )\n",
    "            \n",
    "            yield {\n",
    "                \"user_id\": row['user_id:token'],\n",
    "                \"job_id\": row['job_id:token'],\n",
    "                \"model_input\": {\n",
    "                    \"model_input_cv\": tokenized_cv,\n",
    "                    \"model_input_jd\": tokenized_jd,\n",
    "                    \"label\": torch.tensor(row['browsed:label'], dtype=torch.float32)\n",
    "                }\n",
    "            }\n",
    "\n",
    "train_dataset = BGE_FTDataset('train', 'dataset/processed_train.csv', tokenizer,ratio=0.001)\n",
    "valid_dataset = BGE_FTDataset('valid', \"dataset/processed_valid.csv\", tokenizer,ratio=0.001)\n",
    "test_dataset = BGE_FTDataset('test', \"dataset/processed_test.csv\", tokenizer, ratio = 0.01)\n",
    "\n",
    "# train_dataset[0]\n",
    "# valid_dataset[0]\n",
    "# test_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from sklearn.metrics import f1_score, accuracy_score, roc_auc_score, log_loss\n",
    "class Evaluator:\n",
    "    \"\"\"\n",
    "    Evaluator for the BGE-FT model.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        #TODO: the label values are currently only assumed to be binary\n",
    "        # For further experiments, we need to make the label values more general.  \n",
    "        self.uid2topk = {} # {uid: [(score, label), ...]}  \n",
    "        \n",
    "        self.topk = 10\n",
    "        self.metric2func = {\n",
    "            \"ndcg\": self._ndcg,\n",
    "            \"precision\": self._precision,\n",
    "            \"recall\": self._recall,\n",
    "            \"map\": self._map,\n",
    "            \"mrr\": self._mrr,\n",
    "            \"auc\": self._auc,\n",
    "            \"logloss\": self._logloss,\n",
    "        }\n",
    "        self.cls_metrics = [\"auc\", \"logloss\"]\n",
    "        self.rkg_metrics = [\"ndcg\", \"precision\", \"recall\", \"map\", \"mrr\"]\n",
    "        \n",
    "\n",
    "    def collect(self, uid, score, label):\n",
    "        \"\"\"\n",
    "        Process a batch of data. Save the data to the evaluator. \n",
    "        Input params are lists of same length as batch size.\n",
    "        After this func, uid2topk will look like: {uid: [(score, label), ...]}\n",
    "        where each uid has interaction list sorted by score\n",
    "\n",
    "        Args:\n",
    "            uid: list, list of user ids.  \n",
    "            score: list, list of scores.\n",
    "            label: list, list of labels.\n",
    "            \n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        for u, s, l in zip(uid, score, label):\n",
    "            if u not in self.uid2topk:\n",
    "                self.uid2topk[u] = []\n",
    "            self.uid2topk[u].append((s, l)) \n",
    "\n",
    "        for u in self.uid2topk:\n",
    "            self.uid2topk[u] = sorted(self.uid2topk[u], key=lambda x: x[0], reverse=True)\n",
    "         \n",
    "    def evaluate(self, K: List[int]):\n",
    "        \"\"\"\n",
    "        Evaluate the model using the collected data and the pass value k.\n",
    "        Args:\n",
    "            K: List[int], a list of k values for ranking metrics.\n",
    "        \n",
    "        return:\n",
    "            result: dict, a dictionary of evaluation results.\n",
    "            result_str: str, a formatted string of the evaluation results.\n",
    "        \"\"\"\n",
    "        result = {} # {cls_m1: value1, cls_m2: value2, ..., rkg_m1@k1: value1, rkg_m2@k2: value2, ...}\n",
    "\n",
    "        # Calculate the metrics\n",
    "        for cls_metric in self.cls_metrics:\n",
    "            matric_val = self.metric2func[cls_metric]()\n",
    "            result[cls_metric] = matric_val\n",
    "\n",
    "        for rkg_metric in self.rkg_metrics:\n",
    "            for k in K:\n",
    "                result[rkg_metric + '@' + str(k)] = self.metric2func[rkg_metric](k)\n",
    "        \n",
    "        result_str = self._format_str(result)\n",
    "        return result, result_str\n",
    "    \n",
    "\n",
    "    # below are the ranking metric functions. With most of are indirect copy from the recbole.metrics.\n",
    "    def _ndcg(self, k):\n",
    "        base = []\n",
    "        idcg = []\n",
    "\n",
    "        # save base and idcg(Ideal DCG) for each position\n",
    "        for i in range(k):\n",
    "            base.append(np.log(2) / np.log(i + 2)) # np.log(2) / np.log(i + 2) = log_{i + 2}(2)\n",
    "            if i > 0:\n",
    "                idcg.append(base[i] + idcg[i - 1])\n",
    "            else:\n",
    "                idcg.append(base[i])\n",
    "\n",
    "        # calculate the dcg\n",
    "        tot = 0\n",
    "        for uid in self.uid2topk:\n",
    "            dcg = 0\n",
    "            pos = 0\n",
    "            for i, (score, label) in enumerate(self.uid2topk[uid][:k]):\n",
    "                dcg += (2 ** label - 1) * base[i] # 2^rel - 1 / log_(2)(i + 1)\n",
    "                pos += label # TODO: If label is not binary, this should be modified.\n",
    "            tot += dcg / idcg[int(pos) - 1]\n",
    "        return tot / len(self.uid2topk)\n",
    "\n",
    "    def _precision(self, k):\n",
    "        tot = 0\n",
    "        valid_length = 0\n",
    "        for uid in self.uid2topk:\n",
    "            rec = 0\n",
    "            rel = 0\n",
    "            for i, (score, label) in enumerate(self.uid2topk[uid][:k]):\n",
    "                rec += 1\n",
    "                rel += label # TODO: If label is not binary, this should (maybe) be modified.\n",
    "            try:\n",
    "                tot += rel / rec\n",
    "                valid_length += 1\n",
    "            except:\n",
    "                continue\n",
    "        return tot / valid_length\n",
    "    \n",
    "    def _recall(self, k):\n",
    "        tot = 0\n",
    "        valid_length = 0\n",
    "        for uid in self.uid2topk:\n",
    "            rec = 0\n",
    "            rel = 0\n",
    "            for i, (score, label) in enumerate(self.uid2topk[uid]):\n",
    "                if i < k:\n",
    "                    rec += label\n",
    "                rel += label #TODO: If label is not binary, this should (maybe) be modified.\n",
    "            try:\n",
    "                tot += rec / rel\n",
    "                valid_length += 1\n",
    "            except:\n",
    "                continue\n",
    "        return tot / valid_length\n",
    "\n",
    "    # TODO: The MAP and MRR functions are not understood yet.\n",
    "    def _map(self,k):\n",
    "        tot = 0\n",
    "        for uid in self.uid2topk:\n",
    "            tp = 0\n",
    "            pos = 0\n",
    "            ap = 0\n",
    "            for i, (score, label) in enumerate(self.uid2topk[uid][:k]):\n",
    "                if label == 1:\n",
    "                    tp += 1\n",
    "                    pos += 1\n",
    "                    ap += tp / (i + 1)\n",
    "            if pos > 0:\n",
    "                tot += ap / pos\n",
    "        return tot / len(self.uid2topk)\n",
    "\n",
    "    def _mrr(self, k):\n",
    "        tot = 0\n",
    "        for uid in self.uid2topk:\n",
    "            for i, (score, label) in enumerate(self.uid2topk[uid]):\n",
    "                if label == 1:\n",
    "                    tot += 1 / (i + 1)\n",
    "                    break\n",
    "        return tot / len(self.uid2topk)\n",
    "        \n",
    "    \n",
    "    # below are the classification metric functions\n",
    "    def _auc(self):\n",
    "        \"\"\"\n",
    "        Calculate the AUC score.\n",
    "        \"\"\"\n",
    "        total_auc = 0\n",
    "        valid_auc_num = 0\n",
    "        for uid, topk in self.uid2topk.items():\n",
    "            score, labels = zip(*topk)\n",
    "            try:\n",
    "                auc = roc_auc_score(labels, score)\n",
    "                total_auc += auc\n",
    "                valid_auc_num += 1\n",
    "            except:\n",
    "                continue\n",
    "        return total_auc / valid_auc_num\n",
    "        \n",
    "\n",
    "    def _logloss(self):\n",
    "        \"\"\"\n",
    "        Calculate the logloss.\n",
    "        \"\"\"\n",
    "        total_logloss = 0\n",
    "        valid_logloss_num = 0\n",
    "        for uid, topk in self.uid2topk.items():\n",
    "            score, labels = zip(*topk)\n",
    "            try:\n",
    "                logloss = log_loss(labels, score)\n",
    "                valid_logloss_num += 1\n",
    "                total_logloss += logloss\n",
    "            except:\n",
    "                continue\n",
    "        return total_logloss / valid_logloss_num\n",
    "\n",
    "    # other utility functions for evaluator\n",
    "    def _format_str(self, result):\n",
    "        res = ''\n",
    "        for metric in result.keys():\n",
    "            res += '\\n\\t{}:\\t{:.4f}'.format(metric, result[metric])\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-07 00:10:54,446 - INFO - Initializing Model Based on path: /media/wuyuhuan/bge-small-zh\n",
      "2025-01-07 00:10:56,458 - INFO - Frozing Parameters...\n",
      "2025-01-07 00:10:56,464 - INFO - Model Initialized.\n",
      "2025-01-07 00:10:56,477 - INFO - Trainable Params: 266529. Total Params: 48174369. Trainable Paramaters Ratio: 0.005532589331891405\n",
      "Epoch 0 Train: 100%|██████████| 2/2 [00:02<00:00,  1.29s/it, loss=0.535]\n",
      "Epoch 0 Valid: 100%|██████████| 1/1 [00:00<00:00,  3.34it/s, loss=0.527]\n",
      "2025-01-07 00:10:59,394 - INFO - Epoch 0 Train mean Loss: 0.5645, Valid mean Loss: 0.5274\n",
      "Epoch 1 Train: 100%|██████████| 2/2 [00:02<00:00,  1.19s/it, loss=0.508]\n",
      "Epoch 1 Valid: 100%|██████████| 1/1 [00:00<00:00,  3.30it/s, loss=0.525]\n",
      "2025-01-07 00:11:02,106 - INFO - Epoch 1 Train mean Loss: 0.5207, Valid mean Loss: 0.5252\n",
      "2025-01-07 00:11:02,107 - INFO - Epoch 2 starts early stopping check.\n",
      "Epoch 2 Train: 100%|██████████| 2/2 [00:02<00:00,  1.21s/it, loss=0.509]\n",
      "Epoch 2 Valid: 100%|██████████| 1/1 [00:00<00:00,  3.29it/s, loss=0.535]\n",
      "2025-01-07 00:11:04,846 - INFO - Epoch 2 Train mean Loss: 0.5221, Valid mean Loss: 0.5347\n",
      "Epoch 3 Train: 100%|██████████| 2/2 [00:02<00:00,  1.21s/it, loss=0.506]\n",
      "Epoch 3 Valid: 100%|██████████| 1/1 [00:00<00:00,  3.28it/s, loss=0.533]\n",
      "2025-01-07 00:11:07,597 - INFO - Epoch 3 Train mean Loss: 0.5275, Valid mean Loss: 0.5333\n",
      "2025-01-07 00:11:07,598 - INFO - Epoch 4 starts early stopping check.\n",
      "Epoch 4 Train: 100%|██████████| 2/2 [00:02<00:00,  1.19s/it, loss=0.501]\n",
      "Epoch 4 Valid: 100%|██████████| 1/1 [00:00<00:00,  3.36it/s, loss=0.525]\n",
      "2025-01-07 00:11:10,295 - INFO - Epoch 4 Train mean Loss: 0.5198, Valid mean Loss: 0.5247\n",
      "Epoch 5 Train: 100%|██████████| 2/2 [00:02<00:00,  1.19s/it, loss=0.504]\n",
      "Epoch 5 Valid: 100%|██████████| 1/1 [00:00<00:00,  3.31it/s, loss=0.519]\n",
      "2025-01-07 00:11:12,992 - INFO - Epoch 5 Train mean Loss: 0.5209, Valid mean Loss: 0.5186\n",
      "2025-01-07 00:11:12,993 - INFO - Epoch 6 starts early stopping check.\n",
      "Epoch 6 Train: 100%|██████████| 2/2 [00:02<00:00,  1.25s/it, loss=0.492]\n",
      "Epoch 6 Valid: 100%|██████████| 1/1 [00:00<00:00,  3.29it/s, loss=0.519]\n",
      "2025-01-07 00:11:15,820 - INFO - Epoch 6 Train mean Loss: 0.5098, Valid mean Loss: 0.5186\n",
      "Epoch 7 Train: 100%|██████████| 2/2 [00:02<00:00,  1.20s/it, loss=0.499]\n",
      "Epoch 7 Valid: 100%|██████████| 1/1 [00:00<00:00,  3.33it/s, loss=0.521]\n",
      "2025-01-07 00:11:18,531 - INFO - Epoch 7 Train mean Loss: 0.5145, Valid mean Loss: 0.5210\n",
      "2025-01-07 00:11:18,532 - INFO - Epoch 8 starts early stopping check.\n",
      "Epoch 8 Train: 100%|██████████| 2/2 [00:02<00:00,  1.20s/it, loss=0.496]\n",
      "Epoch 8 Valid: 100%|██████████| 1/1 [00:00<00:00,  3.31it/s, loss=0.52]\n",
      "2025-01-07 00:11:21,246 - INFO - Epoch 8 Train mean Loss: 0.5105, Valid mean Loss: 0.5201\n",
      "Epoch 9 Train: 100%|██████████| 2/2 [00:02<00:00,  1.20s/it, loss=0.495]\n",
      "Epoch 9 Valid: 100%|██████████| 1/1 [00:00<00:00,  3.38it/s, loss=0.518]\n",
      "2025-01-07 00:11:23,956 - INFO - Epoch 9 Train mean Loss: 0.5097, Valid mean Loss: 0.5177\n",
      "2025-01-07 00:11:23,957 - INFO - Epoch 10 starts early stopping check.\n",
      "Epoch 10 Train: 100%|██████████| 2/2 [00:02<00:00,  1.19s/it, loss=0.497]\n",
      "Epoch 10 Valid: 100%|██████████| 1/1 [00:00<00:00,  3.34it/s, loss=0.517]\n",
      "2025-01-07 00:11:26,652 - INFO - Epoch 10 Train mean Loss: 0.5101, Valid mean Loss: 0.5175\n",
      "Epoch 11 Train: 100%|██████████| 2/2 [00:02<00:00,  1.19s/it, loss=0.489]\n",
      "Epoch 11 Valid: 100%|██████████| 1/1 [00:00<00:00,  3.34it/s, loss=0.519]\n",
      "2025-01-07 00:11:29,348 - INFO - Epoch 11 Train mean Loss: 0.5013, Valid mean Loss: 0.5190\n",
      "2025-01-07 00:11:29,349 - INFO - Epoch 12 starts early stopping check.\n",
      "Epoch 12 Train: 100%|██████████| 2/2 [00:02<00:00,  1.19s/it, loss=0.489]\n",
      "Epoch 12 Valid: 100%|██████████| 1/1 [00:00<00:00,  3.35it/s, loss=0.519]\n",
      "2025-01-07 00:11:32,046 - INFO - Epoch 12 Train mean Loss: 0.5046, Valid mean Loss: 0.5194\n",
      "Epoch 13 Train: 100%|██████████| 2/2 [00:02<00:00,  1.21s/it, loss=0.485]\n",
      "Epoch 13 Valid: 100%|██████████| 1/1 [00:00<00:00,  3.37it/s, loss=0.518]\n",
      "2025-01-07 00:11:34,770 - INFO - Epoch 13 Train mean Loss: 0.5027, Valid mean Loss: 0.5177\n",
      "2025-01-07 00:11:34,771 - INFO - Epoch 14 starts early stopping check.\n",
      "Epoch 14 Train: 100%|██████████| 2/2 [00:02<00:00,  1.20s/it, loss=0.487]\n",
      "Epoch 14 Valid: 100%|██████████| 1/1 [00:00<00:00,  3.31it/s, loss=0.516]\n",
      "2025-01-07 00:11:37,495 - INFO - Epoch 14 Train mean Loss: 0.5015, Valid mean Loss: 0.5160\n",
      "Epoch 15 Train: 100%|██████████| 2/2 [00:02<00:00,  1.20s/it, loss=0.494]\n",
      "Epoch 15 Valid: 100%|██████████| 1/1 [00:00<00:00,  3.29it/s, loss=0.516]\n",
      "2025-01-07 00:11:40,222 - INFO - Epoch 15 Train mean Loss: 0.5024, Valid mean Loss: 0.5159\n",
      "2025-01-07 00:11:40,223 - INFO - Epoch 16 starts early stopping check.\n",
      "Epoch 16 Train: 100%|██████████| 2/2 [00:02<00:00,  1.20s/it, loss=0.483]\n",
      "Epoch 16 Valid: 100%|██████████| 1/1 [00:00<00:00,  3.28it/s, loss=0.516]\n",
      "2025-01-07 00:11:42,937 - INFO - Epoch 16 Train mean Loss: 0.4975, Valid mean Loss: 0.5164\n",
      "Epoch 17 Train: 100%|██████████| 2/2 [00:02<00:00,  1.19s/it, loss=0.483]\n",
      "Epoch 17 Valid: 100%|██████████| 1/1 [00:00<00:00,  3.35it/s, loss=0.516]\n",
      "2025-01-07 00:11:45,631 - INFO - Epoch 17 Train mean Loss: 0.4970, Valid mean Loss: 0.5160\n",
      "2025-01-07 00:11:45,632 - INFO - Epoch 18 starts early stopping check.\n",
      "Epoch 18 Train: 100%|██████████| 2/2 [00:02<00:00,  1.20s/it, loss=0.485]\n",
      "Epoch 18 Valid: 100%|██████████| 1/1 [00:00<00:00,  3.35it/s, loss=0.515]\n",
      "2025-01-07 00:11:48,344 - INFO - Epoch 18 Train mean Loss: 0.4984, Valid mean Loss: 0.5155\n",
      "Epoch 19 Train: 100%|██████████| 2/2 [00:02<00:00,  1.19s/it, loss=0.481]\n",
      "Epoch 19 Valid: 100%|██████████| 1/1 [00:00<00:00,  3.31it/s, loss=0.516]\n",
      "2025-01-07 00:11:51,045 - INFO - Epoch 19 Train mean Loss: 0.4952, Valid mean Loss: 0.5157\n",
      "2025-01-07 00:11:51,046 - INFO - Epoch 20 starts early stopping check.\n",
      "Epoch 20 Train: 100%|██████████| 2/2 [00:02<00:00,  1.48s/it, loss=0.475]\n",
      "Epoch 20 Valid: 100%|██████████| 1/1 [00:00<00:00,  3.03it/s, loss=0.516]\n",
      "2025-01-07 00:11:54,358 - INFO - Epoch 20 Train mean Loss: 0.4886, Valid mean Loss: 0.5159\n",
      "Epoch 21 Train: 100%|██████████| 2/2 [00:02<00:00,  1.27s/it, loss=0.481]\n",
      "Epoch 21 Valid: 100%|██████████| 1/1 [00:00<00:00,  2.85it/s, loss=0.516]\n",
      "2025-01-07 00:11:57,268 - INFO - Epoch 21 Train mean Loss: 0.4904, Valid mean Loss: 0.5159\n",
      "2025-01-07 00:11:57,269 - INFO - Epoch 22 starts early stopping check.\n",
      "2025-01-07 00:11:57,270 - INFO - Early stopping at epoch 21\n",
      "2025-01-07 00:11:57,272 - INFO - Start evaluating on test set\n",
      "Matrices Evaluation     :   0%|          | 0/3 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "tuple indices must be integers or slices, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 273\u001b[0m\n\u001b[1;32m    271\u001b[0m best_valid \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mtrain(epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1000\u001b[39m)\n\u001b[1;32m    272\u001b[0m evaluator \u001b[38;5;241m=\u001b[39m Evaluator()\n\u001b[0;32m--> 273\u001b[0m result, result_str \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39meval(evaluator)\n",
      "File \u001b[0;32m~/anaconda3/envs/recbole/lib/python3.12/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "Cell \u001b[0;32mIn[24], line 160\u001b[0m, in \u001b[0;36mTrainer.eval\u001b[0;34m(self, evaluator)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;66;03m# predicting scores, while saving the predictions records.\u001b[39;00m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m iter_data:\n\u001b[0;32m--> 160\u001b[0m     uid \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser_id\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;66;03m# List of length bs\u001b[39;00m\n\u001b[1;32m    161\u001b[0m     batch_inputs \u001b[38;5;241m=\u001b[39m {k: v\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_input\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mitems()} \u001b[38;5;66;03m# batch: {\"model_input_jd\", \"model_input_cv\": dict, \"label\": tensor}\u001b[39;00m\n\u001b[1;32m    162\u001b[0m     scores \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(batch_inputs)\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mtolist()\n",
      "\u001b[0;31mTypeError\u001b[0m: tuple indices must be integers or slices, not str"
     ]
    }
   ],
   "source": [
    "collate_fn = lambda batch: {\n",
    "        \"user_id\": [item[\"user_id\"] for item in batch],\n",
    "        \"job_id\": [item[\"job_id\"] for item in batch],\n",
    "        \"model_input\": {\n",
    "            \"model_input_cv\": {\n",
    "                k: torch.cat([item[\"model_input\"][\"model_input_cv\"][k] for item in batch]) \n",
    "                for k in batch[0][\"model_input\"][\"model_input_cv\"]\n",
    "            },\n",
    "            \"model_input_jd\": {\n",
    "                k: torch.cat([item[\"model_input\"][\"model_input_jd\"][k] for item in batch])\n",
    "                for k in batch[0][\"model_input\"][\"model_input_jd\"]\n",
    "            },\n",
    "            \"label\": torch.stack([item[\"model_input\"][\"label\"] for item in batch])\n",
    "        }\n",
    "    }\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=256, shuffle=False,num_workers=0, collate_fn=collate_fn)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=256, shuffle=False,num_workers=0, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=256, shuffle=False,num_workers=0, collate_fn=collate_fn)\n",
    "\n",
    "class BGE_FTModel(torch.nn.Module):\n",
    "    def __init__(self, rag_model):\n",
    "        super(BGE_FTModel, self).__init__()\n",
    "        logging.info(f\"Initializing Model Based on path: {rag_model}\")\n",
    "        self.jd_retriever = AutoModel.from_pretrained(rag_model).to(device)\n",
    "        self.cv_retriever = AutoModel.from_pretrained(rag_model).to(device)\n",
    "        self.predictor = nn.Sequential(\n",
    "            nn.Linear(1024, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 1),\n",
    "            nn.Sigmoid()\n",
    "        ).to(device)\n",
    "\n",
    "        self.loss_fn = nn.BCELoss()\n",
    "        # xavier initialization for predictor\n",
    "        for m in self.predictor:\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "        \n",
    "        logging.info(f\"Frozing Parameters...\")\n",
    "        self.frozen_target_parameters()\n",
    "        logging.info(f\"Model Initialized.\")\n",
    "        self.print_trainable_parameters()\n",
    "\n",
    "\n",
    "    def forward(self, sample):\n",
    "        \"\"\"sample: dict like {\n",
    "            \"model_input_jd\": {\"input_ids\": tensor, \"attention_mask\": tensor, \"token_type_ids\": tensor},\n",
    "            \"model_input_cv\": {\"input_ids\": tensor, \"attention_mask\": tensor, \"token_type_ids\": tensor}\n",
    "            \"label\": tensor\n",
    "        }\n",
    "        \"\"\"\n",
    "        jd = {k: v.squeeze(1) for k, v in sample[\"model_input_jd\"].items()}\n",
    "        # (input_ids: tensor(bs, seq_len), attention_mask: tensor(bs, seq_len), token_type_ids: tensor(bs, seq_len))\n",
    "        cv = {k: v.squeeze(1) for k, v in sample[\"model_input_cv\"].items()}\n",
    "        jd_output = self.jd_retriever(**jd)[0][:, 0]\n",
    "        cv_output = self.cv_retriever(**cv)[0][:, 0] # (bs, seq_len)\n",
    "        # concat jd and cv\n",
    "        concat_output = torch.cat((jd_output, cv_output), 1)\n",
    "        return self.predictor(concat_output)\n",
    "    \n",
    "    def calculate_loss(self, output, label):\n",
    "        #TODO: Apply more innovative loss functions.\n",
    "        return self.loss_fn(output, label)\n",
    "\n",
    "    def frozen_target_parameters(self):\n",
    "        for param in self.jd_retriever.parameters():\n",
    "            param.requires_grad = False\n",
    "        for param in self.cv_retriever.parameters():\n",
    "            param.requires_grad = False\n",
    "    \n",
    "    def print_trainable_parameters(self):\n",
    "        total_params = sum(p.numel() for p in self.parameters())\n",
    "        trainable_params = sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "        logging.info(f\"Trainable Params: {trainable_params}. Total Params: {total_params}. Trainable Paramaters Ratio: {trainable_params/total_params}\")\n",
    "\n",
    "class Trainer(object):\n",
    "    def __init__(self, model, train_dataloader, valid_dataloader, test_dataloader, optimizer, eval_step, verbose=True):\n",
    "        self.model = model\n",
    "        self.train_dataloader = train_dataloader\n",
    "        self.valid_dataloader = valid_dataloader\n",
    "        self.test_dataloader = test_dataloader\n",
    "        self.clip_grad_norm = None\n",
    "        self.optimizer = optimizer\n",
    "        self.eval_step = 2\n",
    "\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def train(self, epochs, early_stopping_epochs=10):\n",
    "        train_loss = valid_loss = float('inf')\n",
    "        \n",
    "        # the below init values are for early stopping\n",
    "        best_valid = cur_best_valid = float('inf')\n",
    "        cur_step_from_best_val = 0\n",
    "        \n",
    "        for epoch_idx in range(epochs):\n",
    "            # train\n",
    "            train_loss = self._train_epoch(epoch_idx, self.train_dataloader) # mean loss of this epoch\n",
    "\n",
    "            # valid\n",
    "            valid_loss = self._valid_epoch(epoch_idx, self.valid_dataloader) # mean loss of this epoch\n",
    "            if self.verbose:\n",
    "                logging.info(f\"Epoch {epoch_idx} Train mean Loss: {train_loss:.4f}, Valid mean Loss: {valid_loss:.4f}\")\n",
    "                \n",
    "            # early stopping\n",
    "            if (epoch_idx + 1) % self.eval_step == 0:\n",
    "                if self.verbose:\n",
    "                    logging.info(f\"Epoch {epoch_idx + 1} starts early stopping check.\")\n",
    "                cur_best_valid, cur_step_from_best_val, stop_flag, update_flag = self._early_stopping(\n",
    "                    valid_loss, cur_best_valid, cur_step_from_best_val, early_stopping_epochs)\n",
    "                \n",
    "                if update_flag:\n",
    "                    best_valid = cur_best_valid\n",
    "            \n",
    "                if stop_flag:\n",
    "                    if self.verbose:\n",
    "                        logging.info(f\"Early stopping at epoch {epoch_idx}\")\n",
    "                    break\n",
    "        \n",
    "        return best_valid\n",
    "\n",
    "    \n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def eval(self, evaluator):\n",
    "        \"\"\"\n",
    "        Using the test dataloader to evaluate the model.\n",
    "        For each (cv, jd) pair, we predict the probability of the cv being browsed.\n",
    "        The evaluation results are saved to the save_path as the following format:\n",
    "\n",
    "        The evaluation matriceare all based on top-k selection. for each cv_i, the \n",
    "        top-k are selected from all (cv_i, jd) pairs that appear in the test set. \n",
    "        After consideration, due to the context of precise-recommendation matching \n",
    "        task, we decide if jd_j are in the testset records but not being recorded \n",
    "        with cv_i in the testset, we will not consider jd_j in the top-k selection\n",
    "        for cv_i.\n",
    "        \n",
    "        params:\n",
    "            evaluator: Evaluator, the evaluator for the model.\n",
    "\n",
    "        return:\n",
    "            result: list\n",
    "        \"\"\"\n",
    "        # set model to eval mode\n",
    "        if self.verbose:\n",
    "            logging.info(\"Start evaluating on test set\")\n",
    "        self.model.eval()\n",
    "\n",
    "\n",
    "        pbar = tqdm(enumerate(self.test_dataloader), total=len(self.test_dataloader), desc=\"Matrices Evaluation     \")\n",
    "       \n",
    "        # predicting scores, while saving the predictions records.\n",
    "        for step, batch in pbar:\n",
    "            uid = batch[\"user_id\"] # List of length bs\n",
    "            batch_inputs = dict2device(batch[\"model_input\"]) # {\"model_input_jd\": dict, \"model_input_cv\": dict, \"label\": tensor}\n",
    "            scores = self.model(batch_inputs).squeeze(-1).cpu().tolist()\n",
    "            labels = batch_inputs[\"label\"].squeeze(-1).cpu().tolist()\n",
    "            evaluator.collect(uid, scores, labels)\n",
    "\n",
    "        # evaluate the results\n",
    "        results, results_str = evaluator.evaluate([1, 5, 10])\n",
    "        return results, results_str\n",
    "\n",
    "    # below is indirect copy from https://github.com/hyp1231/SHPJF/tree/master/model\n",
    "    def _train_epoch(self, epoch_idx: int, train_dataloader: DataLoader):\n",
    "        \"\"\"Train the model in an epoch\n",
    "\n",
    "        Args:\n",
    "            epoch_idx (int): The current epoch id.\n",
    "            train_data (DataLoader): The train data.\n",
    "\n",
    "        Returns:\n",
    "            float/tuple: The sum of loss returned by all batches in this epoch. If the loss in each batch contains\n",
    "            multiple parts and the model return these multiple parts loss instead of the sum of loss, it will return a\n",
    "            tuple which includes the sum of loss in each part.\n",
    "        \"\"\"\n",
    "        self.model.train()\n",
    "        total_loss = 0\n",
    "        total_batches = len(train_dataloader) \n",
    "    \n",
    "        pbar = tqdm(enumerate(train_dataloader), total=total_batches, desc=f\"Epoch {epoch_idx} Train\")\n",
    "\n",
    "        for step, batch in pbar: # batch: {\"user_id\": tensor, \"job_id\": tensor, \"model_input\": dict}\n",
    "            batch = dict2device(batch[\"model_input\"], device) # batch: {\"model_input_jd\": dict, \"model_input_cv\": dict, \"label\": tensor}\n",
    "            label = batch[\"label\"].unsqueeze(1).to(device) # (bs, 1)\n",
    "            self.optimizer.zero_grad()\n",
    "            output = self.model(batch) # tensor(bs, 1)\n",
    "            loss = self.model.calculate_loss(output, label) # output: (bs, 1), label: (bs, 1)\n",
    "            pbar.set_postfix(loss=loss.item())\n",
    "            total_loss += loss.item()\n",
    "            self._check_nan(loss)\n",
    "\n",
    "            loss.backward()\n",
    "            if self.clip_grad_norm:\n",
    "                clip_grad_norm_(self.model.parameters(), **self.clip_grad_norm)\n",
    "            self.optimizer.step()\n",
    "    \n",
    "        return total_loss / total_batches\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def _valid_epoch(self, epoch_idx: int, valid_dataloader: DataLoader):\n",
    "        \"\"\"valid the model with valid data by calculate the loss\n",
    "        \"\"\"\n",
    "        \n",
    "        # set model to eval mode\n",
    "        self.model.eval()\n",
    "        total_loss = 0\n",
    "        total_batches = len(valid_dataloader)\n",
    "        pbar = tqdm(enumerate(valid_dataloader), total=total_batches, desc=f\"Epoch {epoch_idx} Valid\")\n",
    "\n",
    "        # calculate loss on validation set\n",
    "        for step, batch in pbar:\n",
    "            batch = dict2device(batch[\"model_input\"], device) # batch: {\"model_input_jd\": dict, \"model_input_cv\": dict, \"label\": tensor}\n",
    "            label = batch[\"label\"].unsqueeze(1) # (bs, 1)\n",
    "            output = self.model(batch) # (bs, 1)\n",
    "            loss = self.model.calculate_loss(output, label) # output: (bs, 1), label: (bs, 1)\n",
    "            pbar.set_postfix(loss=loss.item())\n",
    "            total_loss += loss.item()\n",
    "            self._check_nan(loss)\n",
    "\n",
    "        return total_loss / total_batches\n",
    "    \n",
    "    def _early_stopping(self, value, best, cur_step, max_step):\n",
    "        \"\"\"validation-based early stopping\n",
    "\n",
    "        Args:\n",
    "            value (float): current result\n",
    "            best (float): best result\n",
    "            cur_step (int): the number of consecutive steps that did not exceed the best result\n",
    "            max_step (int): threshold steps for stopping\n",
    "\n",
    "        Returns:\n",
    "            tuple:\n",
    "            - best: float,\n",
    "            best result after this step\n",
    "            - cur_step: int,\n",
    "            the number of consecutive steps that did not exceed the best result after this step\n",
    "            - stop_flag: bool,\n",
    "            whether to stop\n",
    "            - update_flag: bool,\n",
    "            whether to update\n",
    "        \"\"\"\n",
    "        stop_flag = False\n",
    "        update_flag = False\n",
    "        if value > best:\n",
    "            cur_step = 0\n",
    "            best = value\n",
    "            update_flag = True\n",
    "        else:\n",
    "            cur_step += 1\n",
    "            if cur_step > max_step:\n",
    "                stop_flag = True\n",
    "        return best, cur_step, stop_flag, update_flag\n",
    "\n",
    "    def _check_nan(self, loss):\n",
    "        if torch.isnan(loss).any():\n",
    "            raise ValueError(\"Model diverged with loss = NaN\")\n",
    "        return\n",
    "\n",
    "    \n",
    "model = BGE_FTModel(bge_path).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "loss_fn = nn.BCELoss()\n",
    "trainer = Trainer(model, train_loader, valid_loader, test_loader, optimizer, eval_step=1)\n",
    "best_valid = trainer.train(epochs = 1000)\n",
    "evaluator = Evaluator()\n",
    "result, result_str = trainer.eval(evaluator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'auc': 0.5222759856630824,\n",
       " 'logloss': 0.8134653517391849,\n",
       " 'ndcg@1': 0.2115987460815047,\n",
       " 'ndcg@5': 0.2638678062198084,\n",
       " 'ndcg@10': 0.26386336877019884,\n",
       " 'precision@1': 0.2115987460815047,\n",
       " 'precision@5': 0.2082462556600487,\n",
       " 'precision@10': 0.20828606259640736,\n",
       " 'recall@1': 0.6560869565217391,\n",
       " 'recall@5': 0.9950724637681161,\n",
       " 'recall@10': 1.0,\n",
       " 'map@1': 0.2115987460815047,\n",
       " 'map@5': 0.2509106873331011,\n",
       " 'map@10': 0.2506021835763215,\n",
       " 'mrr@1': 0.25172040603075085,\n",
       " 'mrr@5': 0.25172040603075085,\n",
       " 'mrr@10': 0.25172040603075085}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{'auc': 0.49484767025089593,\n",
    " 'logloss': 0.8752708701473016,\n",
    " 'ndcg@1': 0.20637408568443052,\n",
    " 'precision@1': 0.20637408568443052,\n",
    " 'recall@1': 0.6475362318840581,\n",
    " 'map@1': 0.20637408568443052,\n",
    " 'mrr@1': 0.24705304274269782}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BGE_FTModel(\n",
      "  (jd_retriever): BertModel(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(21128, 512, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 512)\n",
      "      (token_type_embeddings): Embedding(2, 512)\n",
      "      (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-3): 4 x BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSdpaSelfAttention(\n",
      "              (query): Linear(in_features=512, out_features=512, bias=True)\n",
      "              (key): Linear(in_features=512, out_features=512, bias=True)\n",
      "              (value): Linear(in_features=512, out_features=512, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=512, out_features=512, bias=True)\n",
      "              (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=2048, out_features=512, bias=True)\n",
      "            (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): BertPooler(\n",
      "      (dense): Linear(in_features=512, out_features=512, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (cv_retriever): BertModel(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(21128, 512, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 512)\n",
      "      (token_type_embeddings): Embedding(2, 512)\n",
      "      (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-3): 4 x BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSdpaSelfAttention(\n",
      "              (query): Linear(in_features=512, out_features=512, bias=True)\n",
      "              (key): Linear(in_features=512, out_features=512, bias=True)\n",
      "              (value): Linear(in_features=512, out_features=512, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=512, out_features=512, bias=True)\n",
      "              (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=2048, out_features=512, bias=True)\n",
      "            (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): BertPooler(\n",
      "      (dense): Linear(in_features=512, out_features=512, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (predictor): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=256, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=256, out_features=16, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=16, out_features=1, bias=True)\n",
      "    (5): ReLU()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "jd will look like this after decoding: \n",
    "\n",
    "[CLS] 岗 位 职 责 ： 负 责 公 司 各 类 图 书 的 选 题 、 组 稿 、 编 辑 、 审 读 工 作 。 任 职 要 求 ： 1 、 正 规 院 校 一 类 本 科 及 以 上 学 历 ， 英 语 及 相 关 专 业 毕 业 ， 有 专 业 八 级 证 书 ； 2 、 综 合 素 质 佳 ， 表 达 流 畅 ； 3 、 有 较 强 的 责 任 心 及 较 好 的 抗 压 能 力 ； 4 、 心 态 好 ， 繁 忙 工 作 中 依 然 能 尽 职 尽 责 的 完 成 自 己 的 工 作 任 务 ； 5 、 有 初 、 高 中 家 教 工 作 经 验 或 教 师 工 作 经 验 者 尤 佳 ； 6 、 我 们 需 要 的 是 热 爱 教 育 及 出 版 行 业 的 有 志 之 士 ， 认 真 踏 实 ， 求 真 务 实 ， 与 天 星 共 荣 辱 ， 同 创 辉 煌 ！ 福 利 待 遇 ： 1. 六 险 一 金 齐 全 （ 养 老 、 医 疗 、 失 业 、 工 伤 、 生 育 、 住 房 公 积 金 ） ； 2. 五 天 工 作 制 ， 享 受 国 家 法 定 节 假 日 及 各 项 过 节 福 利 ； 3. 丰 厚 的 季 度 奖 和 年 终 奖 ， 并 为 员 工 提 供 每 年 健 康 体 检 ； 4. 带 薪 假 期 （ 年 假 、 婚 假 、 病 假 、 产 假 等 ） ； 5. 每 年 为 员 工 订 阅 上 万 本 各 类 期 刊 杂 志 和 图 书 ， 提 供 再 教 育 补 贴 ； 6. 旅 游 ， 聚 餐 及 各 类 综 艺 活 动 ； 7. 提 供 各 个 岗 位 的 内 部 及 外 派 带 薪 培 训 。 [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
    "\n",
    "cv will look like this after decoding: \n",
    "\n",
    "[CLS] 个 人 简 历 : 期 望 行 业 : 教 育 / 培 训 / 院 校, 贸 易 / 进 出 口, 旅 游 / 度 假 教 育 / 培 训, 旅 游 / 度 假 / 出 入 境 服 务, 贸 易 跟 单 当 前 行 业 : 旅 游 / 度 假 旅 游 / 度 假 / 出 入 境 服 务 工 作 经 历 : 互 联 网 参 考 模 型 osi 七 层 | 外 国 语 言 文 学 | 预 定 | 员 工 | 处 理 | 英 语 教 育 | office | 英 语 | 财 务 部 门 | 商 务 | 酒 店 | 订 票 | 邮 件 | 协 调 | 图 书 | 操 作 | 线 上 | 出 票 | 网 站 | 接 听 | 外 国 | 财 务 | 行 程 | 咨 询 | 协 助 | 企 业 | 编 辑 | 团 队 领 导 | 服 务 | 工 具 | 学 习 能 力 | 出 差 | 英 语 读 写 | 维 护 | 通 讯 | 电 话 | 系 统 | 业 务 | 沟 通 能 力 | 国 际 机 票 操 作 | 客 户 | 规 划 | 机 票 | 订 单 | 国 际 | 大 客 户 | 团 队 | 软 件 | 适 应 能 力 [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "recbole",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
