{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    ".inter\n",
    "(g, j, s)\n",
    "g.info <- .udoc\n",
    "j.info <- .idoc\n",
    "\n",
    "g.strRep -> C \n",
    "            o          M\n",
    "            n --BGE--> L ---> out\n",
    "            c --BGE--> P ---> put\n",
    "            a          s\n",
    "j.strRep -> t\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.12.8 | packaged by Anaconda, Inc. | (main, Dec 11 2024, 16:31:09) [GCC 11.2.0]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-16 14:22:35,228 - INFO - Using BGE model from /media/wuyuhuan/bge-small-zh\n",
      "2025-01-16 14:22:35,231 - INFO - Torch Imported. Using PyTorch version 2.5.1\n",
      "2025-01-16 14:22:37,475 - INFO - Usable GPU: 9\n",
      "2025-01-16 14:22:37,521 - INFO - Setting all seeds to 42 to ensure reproducibility...\n",
      "2025-01-16 14:22:45,712 - INFO - Dataset train Loaded. Shape: (5115, 7)\n",
      "2025-01-16 14:22:46,841 - INFO - Dataset valid Loaded. Shape: (640, 7)\n",
      "2025-01-16 14:22:47,974 - INFO - Dataset test Loaded. Shape: (639, 7)\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "bge_path = \"/media/wuyuhuan/bge-small-zh\"\n",
    "logging.info(f\"Using BGE model from {bge_path}\")\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.clip_grad import clip_grad_norm_\n",
    "logging.info(f\"Torch Imported. Using PyTorch version {torch.__version__}\")\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import peft\n",
    "\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "logging.info(f\"Usable GPU: {torch.cuda.device_count()}\")  \n",
    "tokenizer = AutoTokenizer.from_pretrained(bge_path)\n",
    "\n",
    "def same_seed(seed):\n",
    "    '''Fixes random number generator seeds for reproducibility.'''\n",
    "    logging.info(f\"Setting all seeds to {seed} to ensure reproducibility...\")\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def dict2device(data, device):\n",
    "    for k, v in data.items():\n",
    "        if isinstance(v, dict):\n",
    "            data[k] = dict2device(v, device)\n",
    "        elif isinstance(v, torch.Tensor):\n",
    "            data[k] = v.to(device)\n",
    "    return data\n",
    "\n",
    "same_seed(42)\n",
    "\n",
    "class BGE_FTDataset(Dataset):\n",
    "    def __init__(self, mode: str, in_file: str, tokenizer: AutoTokenizer, ratio: float = 1.0):\n",
    "        \"\"\"\n",
    "        mode: str, one of ['train', 'valid', 'test']\n",
    "        in_file: str, path to the input csv file.\n",
    "        tokenizer: AutoTokenizer, tokenizer for the model.\n",
    "        ratio: float, the ratio of the data to be used. Default: 1.0. \n",
    "            set to 0.01 for functionality testing.\n",
    "        \"\"\"\n",
    "        self.mode = mode\n",
    "        self.dataset = pd.read_csv(in_file).sample(frac=ratio)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(bge_path)\n",
    "        logging.info(f\"Dataset {mode} Loaded. Shape: {self.dataset.shape}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.dataset.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.dataset.iloc[idx]\n",
    "        return {\n",
    "            \"user_id\": row['user_id:token'],\n",
    "            \"job_id\": row['job_id:token'],\n",
    "            \"text_pair\": (row[\"cv\"], row[\"jd\"]),\n",
    "            \"label\": row[\"browsed:label\"]}\n",
    "\n",
    "train_dataset = BGE_FTDataset('train', 'dataset/processed_train.csv', tokenizer,ratio=0.01)\n",
    "valid_dataset = BGE_FTDataset('valid', \"dataset/processed_valid.csv\", tokenizer,ratio=0.01)\n",
    "test_dataset = BGE_FTDataset('test', \"dataset/processed_test.csv\", tokenizer, ratio=0.01)\n",
    "\n",
    "# train_dataset[0]\n",
    "# valid_dataset[0]\n",
    "# test_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch, tokenizer):\n",
    "    user_ids = [item['user_id'] for item in batch]\n",
    "    job_ids = [item['job_id'] for item in batch]\n",
    "    cv_texts, jd_texts = zip(*[item['text_pair'] for item in batch])\n",
    "    labels = [item['label'] for item in batch]\n",
    "\n",
    "    tokenized_cv = tokenizer(text=cv_texts, \n",
    "                             text_pair=jd_texts,\n",
    "                             padding='max_length',\n",
    "                             truncation=True,\n",
    "                             return_tensors='pt')\n",
    "\n",
    "    return {\n",
    "        'user_id':user_ids,\n",
    "        'job_id':job_ids,\n",
    "        'model_input': {\n",
    "            'input_ids': tokenized_cv['input_ids'],\n",
    "            'attention_mask': tokenized_cv['attention_mask'],\n",
    "            'token_type_ids': tokenized_cv['token_type_ids']\n",
    "            }, \n",
    "        'label': torch.tensor(labels, dtype=torch.float32)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from sklearn.metrics import f1_score, accuracy_score, roc_auc_score, log_loss\n",
    "class Evaluator:\n",
    "    \"\"\"\n",
    "    Evaluator for the BGE-FT model.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        #TODO: the label values are currently only assumed to be binary\n",
    "        # For further experiments, we need to make the label values more general.  \n",
    "        self.uid2topk = {} # {uid: [(score, label), ...]}  \n",
    "        \n",
    "        self.topk = 10\n",
    "        self.metric2func = {\n",
    "            \"ndcg\": self._ndcg,\n",
    "            \"precision\": self._precision,\n",
    "            \"recall\": self._recall,\n",
    "            \"map\": self._map,\n",
    "            \"mrr\": self._mrr,\n",
    "            \"auc\": self._auc,\n",
    "            \"logloss\": self._logloss,\n",
    "        }\n",
    "        self.cls_metrics = [\"auc\", \"logloss\"]\n",
    "        self.rkg_metrics = [\"ndcg\", \"precision\", \"recall\", \"map\", \"mrr\"]\n",
    "    \n",
    "    def collect(self, uid, score, label):\n",
    "        \"\"\"\n",
    "        Process a batch of data. Save the data to the evaluator. \n",
    "        Input params are lists of same length as batch size.\n",
    "        After this func, uid2topk will look like: {uid: [(score, label), ...]}\n",
    "        where each uid has interaction list sorted by score\n",
    "\n",
    "        Args:\n",
    "            uid: list, list of user ids.  \n",
    "            score: list, list of scores.\n",
    "            label: list, list of labels.\n",
    "            \n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        for u, s, l in zip(uid, score, label):\n",
    "            if u not in self.uid2topk:\n",
    "                self.uid2topk[u] = []\n",
    "            self.uid2topk[u].append((s, l)) \n",
    "\n",
    "        for u in self.uid2topk:\n",
    "            self.uid2topk[u] = sorted(self.uid2topk[u], key=lambda x: x[0], reverse=True)\n",
    "         \n",
    "    def evaluate(self, K: List[int]):\n",
    "        \"\"\"\n",
    "        Evaluate the model using the collected data and the pass value k.\n",
    "        Args:\n",
    "            K: List[int], a list of k values for ranking metrics.\n",
    "        \n",
    "        return:\n",
    "            result: dict, a dictionary of evaluation results.\n",
    "            result_str: str, a formatted string of the evaluation results.\n",
    "        \"\"\"\n",
    "        result = {} # {cls_m1: value1, cls_m2: value2, ..., rkg_m1@k1: value1, rkg_m2@k2: value2, ...}\n",
    "\n",
    "        # Calculate the metrics\n",
    "        for cls_metric in self.cls_metrics:\n",
    "            matric_val = self.metric2func[cls_metric]()\n",
    "            result[cls_metric] = matric_val\n",
    "\n",
    "        for rkg_metric in self.rkg_metrics:\n",
    "            for k in K:\n",
    "                result[rkg_metric + '@' + str(k)] = self.metric2func[rkg_metric](k)\n",
    "        \n",
    "        result_str = self._format_str(result)\n",
    "        return result, result_str\n",
    "    \n",
    "    # below are the ranking metric functions. With most of are indirect copy from the recbole.metrics.\n",
    "    def _ndcg(self, k):\n",
    "        base = []\n",
    "        idcg = []\n",
    "\n",
    "        # save base and idcg(Ideal DCG) for each position\n",
    "        for i in range(k):\n",
    "            base.append(np.log(2) / np.log(i + 2)) # np.log(2) / np.log(i + 2) = log_{i + 2}(2)\n",
    "            if i > 0:\n",
    "                idcg.append(base[i] + idcg[i - 1])\n",
    "            else:\n",
    "                idcg.append(base[i])\n",
    "\n",
    "        # calculate the dcg\n",
    "        tot = 0\n",
    "        for uid in self.uid2topk:\n",
    "            dcg = 0\n",
    "            pos = 0\n",
    "            for i, (score, label) in enumerate(self.uid2topk[uid][:k]):\n",
    "                dcg += (2 ** label - 1) * base[i] # 2^rel - 1 / log_(2)(i + 1)\n",
    "                pos += label # TODO: If label is not binary, this should be modified.\n",
    "            tot += dcg / idcg[int(pos) - 1]\n",
    "        return tot / len(self.uid2topk)\n",
    "\n",
    "    def _precision(self, k):\n",
    "        tot = 0\n",
    "        valid_length = 0\n",
    "        for uid in self.uid2topk:\n",
    "            rec = 0\n",
    "            rel = 0\n",
    "            for i, (score, label) in enumerate(self.uid2topk[uid][:k]):\n",
    "                rec += 1\n",
    "                rel += label # TODO: If label is not binary, this should (maybe) be modified.\n",
    "            try:\n",
    "                tot += rel / rec\n",
    "                valid_length += 1\n",
    "            except:\n",
    "                continue\n",
    "        return tot / valid_length\n",
    "    \n",
    "    def _recall(self, k):\n",
    "        tot = 0\n",
    "        valid_length = 0\n",
    "        for uid in self.uid2topk:\n",
    "            rec = 0\n",
    "            rel = 0\n",
    "            for i, (score, label) in enumerate(self.uid2topk[uid]):\n",
    "                if i < k:\n",
    "                    rec += label\n",
    "                rel += label #TODO: If label is not binary, this should (maybe) be modified.\n",
    "            try:\n",
    "                tot += rec / rel\n",
    "                valid_length += 1\n",
    "            except:\n",
    "                continue\n",
    "        return tot / valid_length\n",
    "\n",
    "    # TODO: The MAP and MRR functions are not understood yet.\n",
    "    def _map(self,k):\n",
    "        tot = 0\n",
    "        for uid in self.uid2topk:\n",
    "            tp = 0\n",
    "            pos = 0\n",
    "            ap = 0\n",
    "            for i, (score, label) in enumerate(self.uid2topk[uid][:k]):\n",
    "                if label == 1:\n",
    "                    tp += 1\n",
    "                    pos += 1\n",
    "                    ap += tp / (i + 1)\n",
    "            if pos > 0:\n",
    "                tot += ap / pos\n",
    "        return tot / len(self.uid2topk)\n",
    "\n",
    "    def _mrr(self, k):\n",
    "        tot = 0\n",
    "        for uid in self.uid2topk:\n",
    "            for i, (score, label) in enumerate(self.uid2topk[uid]):\n",
    "                if label == 1:\n",
    "                    tot += 1 / (i + 1)\n",
    "                    break\n",
    "        return tot / len(self.uid2topk)\n",
    "           \n",
    "    # below are the classification metric functions\n",
    "    def _auc(self):\n",
    "        \"\"\"\n",
    "        Calculate the AUC score.\n",
    "        \"\"\"\n",
    "        total_auc = 0\n",
    "        valid_auc_num = 0\n",
    "        for uid, topk in self.uid2topk.items():\n",
    "            score, labels = zip(*topk)\n",
    "            try:\n",
    "                auc = roc_auc_score(labels, score)\n",
    "                total_auc += auc\n",
    "                valid_auc_num += 1\n",
    "            except:\n",
    "                continue\n",
    "        return total_auc / valid_auc_num\n",
    "        \n",
    "    def _logloss(self):\n",
    "        \"\"\"\n",
    "        Calculate the logloss.\n",
    "        \"\"\"\n",
    "        total_logloss = 0\n",
    "        valid_logloss_num = 0\n",
    "        for uid, topk in self.uid2topk.items():\n",
    "            score, labels = zip(*topk)\n",
    "            try:\n",
    "                logloss = log_loss(labels, score)\n",
    "                valid_logloss_num += 1\n",
    "                total_logloss += logloss\n",
    "            except:\n",
    "                continue\n",
    "        return total_logloss / valid_logloss_num\n",
    "\n",
    "    # other utility functions for evaluator\n",
    "    def _format_str(self, result):\n",
    "        res = ''\n",
    "        for metric in result.keys():\n",
    "            res += '\\n\\t{}:\\t{:.4f}'.format(metric, result[metric])\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-13 23:51:12,909 - INFO - Initializing Model Based on path: /media/wuyuhuan/bge-small-zh\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-13 23:51:14,370 - INFO - Frozing Parameters...\n",
      "2025-01-13 23:51:14,371 - INFO - Frozing Parameters...\n",
      "2025-01-13 23:51:14,372 - INFO - Trainable Params: 135457. Total Params: 24089377. Trainable Paramaters Ratio: 0.005623101004231035\n",
      "2025-01-13 23:51:14,373 - INFO - Model Initialized.\n",
      "Epoch 0 Train: 100%|██████████| 1000/1000 [10:12<00:00,  1.63it/s, loss: 0.3751 | forward: 5.31 ms | loss: 0.15 ms | backward: 2.36 ms]\n",
      "Epoch 0 Valid: 100%|██████████| 125/125 [01:14<00:00,  1.69it/s, loss=0.503]\n",
      "2025-01-14 00:02:41,231 - INFO - Epoch 0 Train mean Loss: 0.5226, Valid mean Loss: 0.5194\n",
      "2025-01-14 00:02:41,232 - INFO - Epoch 1 starts early stopping check.\n",
      "Epoch 1 Train: 100%|██████████| 1000/1000 [10:12<00:00,  1.63it/s, loss: 0.4692 | forward: 4.67 ms | loss: 0.14 ms | backward: 1.53 ms]\n",
      "Epoch 1 Valid: 100%|██████████| 125/125 [01:13<00:00,  1.69it/s, loss=0.504]\n",
      "2025-01-14 00:14:08,142 - INFO - Epoch 1 Train mean Loss: 0.5211, Valid mean Loss: 0.5180\n",
      "2025-01-14 00:14:08,143 - INFO - Epoch 2 starts early stopping check.\n",
      "Epoch 2 Train: 100%|██████████| 1000/1000 [10:12<00:00,  1.63it/s, loss: 0.5235 | forward: 4.31 ms | loss: 0.13 ms | backward: 1.39 ms]\n",
      "Epoch 2 Valid: 100%|██████████| 125/125 [01:13<00:00,  1.69it/s, loss=0.505]\n",
      "2025-01-14 00:25:34,966 - INFO - Epoch 2 Train mean Loss: 0.5207, Valid mean Loss: 0.5178\n",
      "2025-01-14 00:25:34,967 - INFO - Epoch 3 starts early stopping check.\n",
      "Epoch 3 Train: 100%|██████████| 1000/1000 [10:12<00:00,  1.63it/s, loss: 0.5724 | forward: 4.47 ms | loss: 0.12 ms | backward: 1.46 ms]\n",
      "Epoch 3 Valid: 100%|██████████| 125/125 [01:13<00:00,  1.69it/s, loss=0.506]\n",
      "2025-01-14 00:37:01,716 - INFO - Epoch 3 Train mean Loss: 0.5207, Valid mean Loss: 0.5183\n",
      "2025-01-14 00:37:01,716 - INFO - Epoch 4 starts early stopping check.\n",
      "Epoch 4 Train: 100%|██████████| 1000/1000 [10:13<00:00,  1.63it/s, loss: 0.5778 | forward: 4.62 ms | loss: 0.13 ms | backward: 1.56 ms]\n",
      "Epoch 4 Valid: 100%|██████████| 125/125 [01:13<00:00,  1.69it/s, loss=0.503]\n",
      "2025-01-14 00:48:28,927 - INFO - Epoch 4 Train mean Loss: 0.5205, Valid mean Loss: 0.5177\n",
      "2025-01-14 00:48:28,928 - INFO - Epoch 5 starts early stopping check.\n",
      "Epoch 5 Train: 100%|██████████| 1000/1000 [10:12<00:00,  1.63it/s, loss: 0.6400 | forward: 4.85 ms | loss: 0.12 ms | backward: 1.52 ms]\n",
      "Epoch 5 Valid: 100%|██████████| 125/125 [01:13<00:00,  1.69it/s, loss=0.503]\n",
      "2025-01-14 00:59:55,736 - INFO - Epoch 5 Train mean Loss: 0.5204, Valid mean Loss: 0.5172\n",
      "2025-01-14 00:59:55,737 - INFO - Epoch 6 starts early stopping check.\n",
      "Epoch 6 Train: 100%|██████████| 1000/1000 [10:12<00:00,  1.63it/s, loss: 0.5230 | forward: 4.59 ms | loss: 0.13 ms | backward: 1.37 ms]\n",
      "Epoch 6 Valid: 100%|██████████| 125/125 [01:13<00:00,  1.69it/s, loss=0.503]\n",
      "2025-01-14 01:11:22,249 - INFO - Epoch 6 Train mean Loss: 0.5201, Valid mean Loss: 0.5172\n",
      "2025-01-14 01:11:22,250 - INFO - Epoch 7 starts early stopping check.\n",
      "Epoch 7 Train: 100%|██████████| 1000/1000 [10:12<00:00,  1.63it/s, loss: 0.5238 | forward: 4.60 ms | loss: 0.13 ms | backward: 1.50 ms]\n",
      "Epoch 7 Valid: 100%|██████████| 125/125 [01:13<00:00,  1.69it/s, loss=0.501]\n",
      "2025-01-14 01:22:48,456 - INFO - Epoch 7 Train mean Loss: 0.5199, Valid mean Loss: 0.5171\n",
      "2025-01-14 01:22:48,456 - INFO - Epoch 8 starts early stopping check.\n",
      "Epoch 8 Train: 100%|██████████| 1000/1000 [10:12<00:00,  1.63it/s, loss: 0.6876 | forward: 4.55 ms | loss: 0.13 ms | backward: 1.45 ms]\n",
      "Epoch 8 Valid: 100%|██████████| 125/125 [01:13<00:00,  1.70it/s, loss=0.502]\n",
      "2025-01-14 01:34:14,212 - INFO - Epoch 8 Train mean Loss: 0.5201, Valid mean Loss: 0.5169\n",
      "2025-01-14 01:34:14,213 - INFO - Epoch 9 starts early stopping check.\n",
      "Epoch 9 Train: 100%|██████████| 1000/1000 [10:11<00:00,  1.63it/s, loss: 0.5329 | forward: 4.88 ms | loss: 0.00 ms | backward: 1.35 ms]\n",
      "Epoch 9 Valid: 100%|██████████| 125/125 [01:13<00:00,  1.69it/s, loss=0.499]\n",
      "2025-01-14 01:45:39,666 - INFO - Epoch 9 Train mean Loss: 0.5197, Valid mean Loss: 0.5170\n",
      "2025-01-14 01:45:39,667 - INFO - Epoch 10 starts early stopping check.\n",
      "Epoch 10 Train: 100%|██████████| 1000/1000 [10:11<00:00,  1.64it/s, loss: 0.3658 | forward: 4.37 ms | loss: 0.12 ms | backward: 1.44 ms]\n",
      "Epoch 10 Valid: 100%|██████████| 125/125 [01:13<00:00,  1.70it/s, loss=0.502]\n",
      "2025-01-14 01:57:04,766 - INFO - Epoch 10 Train mean Loss: 0.5194, Valid mean Loss: 0.5208\n",
      "2025-01-14 01:57:04,766 - INFO - Epoch 11 starts early stopping check.\n",
      "Epoch 11 Train: 100%|██████████| 1000/1000 [10:12<00:00,  1.63it/s, loss: 0.5515 | forward: 4.45 ms | loss: 0.14 ms | backward: 1.47 ms]\n",
      "Epoch 11 Valid: 100%|██████████| 125/125 [01:13<00:00,  1.69it/s, loss=0.502]\n",
      "2025-01-14 02:08:30,817 - INFO - Epoch 11 Train mean Loss: 0.5194, Valid mean Loss: 0.5169\n",
      "2025-01-14 02:08:30,818 - INFO - Epoch 12 starts early stopping check.\n",
      "Epoch 12 Train: 100%|██████████| 1000/1000 [10:12<00:00,  1.63it/s, loss: 0.5554 | forward: 4.47 ms | loss: 0.13 ms | backward: 0.82 ms]\n",
      "Epoch 12 Valid: 100%|██████████| 125/125 [01:13<00:00,  1.69it/s, loss=0.5]  \n",
      "2025-01-14 02:19:56,927 - INFO - Epoch 12 Train mean Loss: 0.5192, Valid mean Loss: 0.5163\n",
      "2025-01-14 02:19:56,927 - INFO - Epoch 13 starts early stopping check.\n",
      "Epoch 13 Train: 100%|██████████| 1000/1000 [10:12<00:00,  1.63it/s, loss: 0.4692 | forward: 4.25 ms | loss: 0.12 ms | backward: 1.47 ms] \n",
      "Epoch 13 Valid: 100%|██████████| 125/125 [01:13<00:00,  1.69it/s, loss=0.5]  \n",
      "2025-01-14 02:31:23,291 - INFO - Epoch 13 Train mean Loss: 0.5190, Valid mean Loss: 0.5163\n",
      "2025-01-14 02:31:23,292 - INFO - Epoch 14 starts early stopping check.\n",
      "Epoch 14 Train: 100%|██████████| 1000/1000 [10:12<00:00,  1.63it/s, loss: 0.5111 | forward: 4.62 ms | loss: 0.13 ms | backward: 1.54 ms]\n",
      "Epoch 14 Valid: 100%|██████████| 125/125 [01:13<00:00,  1.69it/s, loss=0.502]\n",
      "2025-01-14 02:42:49,706 - INFO - Epoch 14 Train mean Loss: 0.5188, Valid mean Loss: 0.5165\n",
      "2025-01-14 02:42:49,707 - INFO - Epoch 15 starts early stopping check.\n",
      "Epoch 15 Train: 100%|██████████| 1000/1000 [10:12<00:00,  1.63it/s, loss: 0.5261 | forward: 4.88 ms | loss: 0.14 ms | backward: 1.66 ms]\n",
      "Epoch 15 Valid: 100%|██████████| 125/125 [01:13<00:00,  1.69it/s, loss=0.501]\n",
      "2025-01-14 02:54:16,102 - INFO - Epoch 15 Train mean Loss: 0.5187, Valid mean Loss: 0.5161\n",
      "2025-01-14 02:54:16,102 - INFO - Epoch 16 starts early stopping check.\n",
      "Epoch 16 Train: 100%|██████████| 1000/1000 [10:12<00:00,  1.63it/s, loss: 0.5129 | forward: 4.53 ms | loss: 0.13 ms | backward: 1.76 ms]\n",
      "Epoch 16 Valid: 100%|██████████| 125/125 [01:13<00:00,  1.69it/s, loss=0.503]\n",
      "2025-01-14 03:05:42,142 - INFO - Epoch 16 Train mean Loss: 0.5185, Valid mean Loss: 0.5159\n",
      "2025-01-14 03:05:42,143 - INFO - Epoch 17 starts early stopping check.\n",
      "Epoch 17 Train: 100%|██████████| 1000/1000 [10:11<00:00,  1.63it/s, loss: 0.5510 | forward: 8.05 ms | loss: 0.23 ms | backward: 1.76 ms]\n",
      "Epoch 17 Valid: 100%|██████████| 125/125 [01:13<00:00,  1.69it/s, loss=0.503]\n",
      "2025-01-14 03:17:08,022 - INFO - Epoch 17 Train mean Loss: 0.5184, Valid mean Loss: 0.5160\n",
      "2025-01-14 03:17:08,023 - INFO - Epoch 18 starts early stopping check.\n",
      "Epoch 18 Train: 100%|██████████| 1000/1000 [10:12<00:00,  1.63it/s, loss: 0.5387 | forward: 4.68 ms | loss: 0.13 ms | backward: 1.77 ms]\n",
      "Epoch 18 Valid: 100%|██████████| 125/125 [01:13<00:00,  1.69it/s, loss=0.502]\n",
      "2025-01-14 03:28:34,175 - INFO - Epoch 18 Train mean Loss: 0.5183, Valid mean Loss: 0.5159\n",
      "2025-01-14 03:28:34,176 - INFO - Epoch 19 starts early stopping check.\n",
      "Epoch 19 Train: 100%|██████████| 1000/1000 [10:11<00:00,  1.63it/s, loss: 0.5023 | forward: 4.26 ms | loss: 0.13 ms | backward: 1.70 ms]\n",
      "Epoch 19 Valid: 100%|██████████| 125/125 [01:13<00:00,  1.69it/s, loss=0.504]\n",
      "2025-01-14 03:39:59,641 - INFO - Epoch 19 Train mean Loss: 0.5180, Valid mean Loss: 0.5161\n",
      "2025-01-14 03:39:59,641 - INFO - Epoch 20 starts early stopping check.\n",
      "Epoch 20 Train: 100%|██████████| 1000/1000 [10:11<00:00,  1.63it/s, loss: 0.4324 | forward: 4.55 ms | loss: 0.13 ms | backward: 1.49 ms]\n",
      "Epoch 20 Valid: 100%|██████████| 125/125 [01:13<00:00,  1.69it/s, loss=0.5]  \n",
      "2025-01-14 03:51:25,307 - INFO - Epoch 20 Train mean Loss: 0.5177, Valid mean Loss: 0.5158\n",
      "2025-01-14 03:51:25,307 - INFO - Epoch 21 starts early stopping check.\n",
      "Epoch 21 Train: 100%|██████████| 1000/1000 [10:11<00:00,  1.63it/s, loss: 0.6603 | forward: 4.78 ms | loss: 0.13 ms | backward: 1.56 ms]\n",
      "Epoch 21 Valid: 100%|██████████| 125/125 [01:13<00:00,  1.69it/s, loss=0.503]\n",
      "2025-01-14 04:02:51,121 - INFO - Epoch 21 Train mean Loss: 0.5179, Valid mean Loss: 0.5154\n",
      "2025-01-14 04:02:51,121 - INFO - Epoch 22 starts early stopping check.\n",
      "Epoch 22 Train: 100%|██████████| 1000/1000 [10:11<00:00,  1.63it/s, loss: 0.6143 | forward: 4.56 ms | loss: 0.13 ms | backward: 1.48 ms]\n",
      "Epoch 22 Valid: 100%|██████████| 125/125 [01:13<00:00,  1.69it/s, loss=0.502]\n",
      "2025-01-14 04:14:16,986 - INFO - Epoch 22 Train mean Loss: 0.5178, Valid mean Loss: 0.5165\n",
      "2025-01-14 04:14:16,987 - INFO - Epoch 23 starts early stopping check.\n",
      "Epoch 23 Train: 100%|██████████| 1000/1000 [10:12<00:00,  1.63it/s, loss: 0.3857 | forward: 4.55 ms | loss: 0.13 ms | backward: 1.41 ms]\n",
      "Epoch 23 Valid: 100%|██████████| 125/125 [01:13<00:00,  1.70it/s, loss=0.503]\n",
      "2025-01-14 04:25:43,187 - INFO - Epoch 23 Train mean Loss: 0.5173, Valid mean Loss: 0.5155\n",
      "2025-01-14 04:25:43,188 - INFO - Epoch 24 starts early stopping check.\n",
      "Epoch 24 Train: 100%|██████████| 1000/1000 [10:12<00:00,  1.63it/s, loss: 0.5149 | forward: 4.62 ms | loss: 0.15 ms | backward: 1.56 ms]\n",
      "Epoch 24 Valid: 100%|██████████| 125/125 [01:13<00:00,  1.69it/s, loss=0.502]\n",
      "2025-01-14 04:37:09,342 - INFO - Epoch 24 Train mean Loss: 0.5173, Valid mean Loss: 0.5154\n",
      "2025-01-14 04:37:09,343 - INFO - Epoch 25 starts early stopping check.\n",
      "Epoch 25 Train: 100%|██████████| 1000/1000 [10:12<00:00,  1.63it/s, loss: 0.4970 | forward: 4.60 ms | loss: 0.14 ms | backward: 1.16 ms]\n",
      "Epoch 25 Valid: 100%|██████████| 125/125 [01:13<00:00,  1.70it/s, loss=0.502]\n",
      "2025-01-14 04:48:35,236 - INFO - Epoch 25 Train mean Loss: 0.5172, Valid mean Loss: 0.5159\n",
      "2025-01-14 04:48:35,236 - INFO - Epoch 26 starts early stopping check.\n",
      "Epoch 26 Train: 100%|██████████| 1000/1000 [10:11<00:00,  1.63it/s, loss: 0.4470 | forward: 4.48 ms | loss: 0.13 ms | backward: 1.46 ms]\n",
      "Epoch 26 Valid: 100%|██████████| 125/125 [01:13<00:00,  1.69it/s, loss=0.505]\n",
      "2025-01-14 05:00:01,073 - INFO - Epoch 26 Train mean Loss: 0.5169, Valid mean Loss: 0.5157\n",
      "2025-01-14 05:00:01,074 - INFO - Epoch 27 starts early stopping check.\n",
      "Epoch 27 Train: 100%|██████████| 1000/1000 [10:11<00:00,  1.64it/s, loss: 0.4558 | forward: 4.64 ms | loss: 0.14 ms | backward: 1.75 ms]\n",
      "Epoch 27 Valid: 100%|██████████| 125/125 [01:13<00:00,  1.69it/s, loss=0.501]\n",
      "2025-01-14 05:11:26,177 - INFO - Epoch 27 Train mean Loss: 0.5169, Valid mean Loss: 0.5153\n",
      "2025-01-14 05:11:26,177 - INFO - Epoch 28 starts early stopping check.\n",
      "Epoch 28 Train: 100%|██████████| 1000/1000 [10:11<00:00,  1.64it/s, loss: 0.4838 | forward: 4.68 ms | loss: 0.13 ms | backward: 1.49 ms]\n",
      "Epoch 28 Valid: 100%|██████████| 125/125 [01:13<00:00,  1.70it/s, loss=0.504]\n",
      "2025-01-14 05:22:51,405 - INFO - Epoch 28 Train mean Loss: 0.5166, Valid mean Loss: 0.5146\n",
      "2025-01-14 05:22:51,405 - INFO - Epoch 29 starts early stopping check.\n",
      "Epoch 29 Train: 100%|██████████| 1000/1000 [10:11<00:00,  1.63it/s, loss: 0.6647 | forward: 4.25 ms | loss: 0.12 ms | backward: 1.42 ms]\n",
      "Epoch 29 Valid: 100%|██████████| 125/125 [01:13<00:00,  1.70it/s, loss=0.504]\n",
      "2025-01-14 05:34:16,909 - INFO - Epoch 29 Train mean Loss: 0.5167, Valid mean Loss: 0.5165\n",
      "2025-01-14 05:34:16,910 - INFO - Epoch 30 starts early stopping check.\n",
      "Epoch 30 Train: 100%|██████████| 1000/1000 [10:11<00:00,  1.64it/s, loss: 0.5464 | forward: 4.39 ms | loss: 0.13 ms | backward: 1.47 ms]\n",
      "Epoch 30 Valid: 100%|██████████| 125/125 [01:13<00:00,  1.70it/s, loss=0.505]\n",
      "2025-01-14 05:45:42,162 - INFO - Epoch 30 Train mean Loss: 0.5164, Valid mean Loss: 0.5150\n",
      "2025-01-14 05:45:42,163 - INFO - Epoch 31 starts early stopping check.\n",
      "Epoch 31 Train: 100%|██████████| 1000/1000 [10:11<00:00,  1.64it/s, loss: 0.4436 | forward: 4.46 ms | loss: 0.13 ms | backward: 1.47 ms]\n",
      "Epoch 31 Valid: 100%|██████████| 125/125 [01:13<00:00,  1.69it/s, loss=0.506]\n",
      "2025-01-14 05:57:07,333 - INFO - Epoch 31 Train mean Loss: 0.5162, Valid mean Loss: 0.5147\n",
      "2025-01-14 05:57:07,334 - INFO - Epoch 32 starts early stopping check.\n",
      "Epoch 32 Train: 100%|██████████| 1000/1000 [10:11<00:00,  1.64it/s, loss: 0.4768 | forward: 4.58 ms | loss: 0.13 ms | backward: 1.55 ms]\n",
      "Epoch 32 Valid: 100%|██████████| 125/125 [01:13<00:00,  1.70it/s, loss=0.507]\n",
      "2025-01-14 06:08:32,399 - INFO - Epoch 32 Train mean Loss: 0.5161, Valid mean Loss: 0.5145\n",
      "2025-01-14 06:08:32,399 - INFO - Epoch 33 starts early stopping check.\n",
      "Epoch 33 Train: 100%|██████████| 1000/1000 [10:10<00:00,  1.64it/s, loss: 0.3940 | forward: 4.02 ms | loss: 0.12 ms | backward: 1.39 ms]\n",
      "Epoch 33 Valid: 100%|██████████| 125/125 [01:13<00:00,  1.70it/s, loss=0.504]\n",
      "2025-01-14 06:19:56,804 - INFO - Epoch 33 Train mean Loss: 0.5159, Valid mean Loss: 0.5144\n",
      "2025-01-14 06:19:56,805 - INFO - Epoch 34 starts early stopping check.\n",
      "Epoch 34 Train: 100%|██████████| 1000/1000 [10:10<00:00,  1.64it/s, loss: 0.4502 | forward: 5.00 ms | loss: 0.14 ms | backward: 1.44 ms]\n",
      "Epoch 34 Valid: 100%|██████████| 125/125 [01:13<00:00,  1.69it/s, loss=0.505]\n",
      "2025-01-14 06:31:21,451 - INFO - Epoch 34 Train mean Loss: 0.5157, Valid mean Loss: 0.5141\n",
      "2025-01-14 06:31:21,451 - INFO - Epoch 35 starts early stopping check.\n",
      "Epoch 35 Train: 100%|██████████| 1000/1000 [10:11<00:00,  1.64it/s, loss: 0.4047 | forward: 4.23 ms | loss: 0.13 ms | backward: 1.53 ms]\n",
      "Epoch 35 Valid: 100%|██████████| 125/125 [01:13<00:00,  1.70it/s, loss=0.503]\n",
      "2025-01-14 06:42:46,338 - INFO - Epoch 35 Train mean Loss: 0.5157, Valid mean Loss: 0.5144\n",
      "2025-01-14 06:42:46,339 - INFO - Epoch 36 starts early stopping check.\n",
      "Epoch 36 Train: 100%|██████████| 1000/1000 [10:10<00:00,  1.64it/s, loss: 0.5834 | forward: 4.45 ms | loss: 0.14 ms | backward: 0.83 ms]\n",
      "Epoch 36 Valid: 100%|██████████| 125/125 [01:13<00:00,  1.70it/s, loss=0.506]\n",
      "2025-01-14 06:54:10,355 - INFO - Epoch 36 Train mean Loss: 0.5156, Valid mean Loss: 0.5144\n",
      "2025-01-14 06:54:10,356 - INFO - Epoch 37 starts early stopping check.\n",
      "Epoch 37 Train: 100%|██████████| 1000/1000 [10:10<00:00,  1.64it/s, loss: 0.6291 | forward: 4.35 ms | loss: 0.12 ms | backward: 1.45 ms]\n",
      "Epoch 37 Valid: 100%|██████████| 125/125 [01:13<00:00,  1.70it/s, loss=0.508]\n",
      "2025-01-14 07:05:34,657 - INFO - Epoch 37 Train mean Loss: 0.5155, Valid mean Loss: 0.5140\n",
      "2025-01-14 07:05:34,658 - INFO - Epoch 38 starts early stopping check.\n",
      "Epoch 38 Train: 100%|██████████| 1000/1000 [10:11<00:00,  1.64it/s, loss: 0.3597 | forward: 4.36 ms | loss: 0.12 ms | backward: 1.60 ms]\n",
      "Epoch 38 Valid: 100%|██████████| 125/125 [01:13<00:00,  1.70it/s, loss=0.509]\n",
      "2025-01-14 07:16:59,449 - INFO - Epoch 38 Train mean Loss: 0.5151, Valid mean Loss: 0.5156\n",
      "2025-01-14 07:16:59,450 - INFO - Epoch 39 starts early stopping check.\n",
      "Epoch 39 Train: 100%|██████████| 1000/1000 [10:11<00:00,  1.64it/s, loss: 0.6460 | forward: 4.42 ms | loss: 0.12 ms | backward: 1.40 ms]\n",
      "Epoch 39 Valid: 100%|██████████| 125/125 [01:13<00:00,  1.70it/s, loss=0.511]\n",
      "2025-01-14 07:28:24,307 - INFO - Epoch 39 Train mean Loss: 0.5153, Valid mean Loss: 0.5144\n",
      "2025-01-14 07:28:24,307 - INFO - Epoch 40 starts early stopping check.\n",
      "Epoch 40 Train: 100%|██████████| 1000/1000 [10:11<00:00,  1.64it/s, loss: 0.5630 | forward: 4.45 ms | loss: 0.13 ms | backward: 1.55 ms]\n",
      "Epoch 40 Valid: 100%|██████████| 125/125 [01:13<00:00,  1.70it/s, loss=0.509]\n",
      "2025-01-14 07:39:49,267 - INFO - Epoch 40 Train mean Loss: 0.5150, Valid mean Loss: 0.5142\n",
      "2025-01-14 07:39:49,268 - INFO - Epoch 41 starts early stopping check.\n",
      "Epoch 41 Train: 100%|██████████| 1000/1000 [10:10<00:00,  1.64it/s, loss: 0.5857 | forward: 4.40 ms | loss: 0.13 ms | backward: 1.48 ms]\n",
      "Epoch 41 Valid: 100%|██████████| 125/125 [01:13<00:00,  1.70it/s, loss=0.508]\n",
      "2025-01-14 07:51:13,872 - INFO - Epoch 41 Train mean Loss: 0.5152, Valid mean Loss: 0.5139\n",
      "2025-01-14 07:51:13,873 - INFO - Epoch 42 starts early stopping check.\n",
      "Epoch 42 Train: 100%|██████████| 1000/1000 [10:10<00:00,  1.64it/s, loss: 0.5238 | forward: 4.41 ms | loss: 0.14 ms | backward: 1.40 ms]\n",
      "Epoch 42 Valid: 100%|██████████| 125/125 [01:13<00:00,  1.69it/s, loss=0.511]\n",
      "2025-01-14 08:02:38,519 - INFO - Epoch 42 Train mean Loss: 0.5149, Valid mean Loss: 0.5140\n",
      "2025-01-14 08:02:38,519 - INFO - Epoch 43 starts early stopping check.\n",
      "Epoch 43 Train: 100%|██████████| 1000/1000 [10:11<00:00,  1.64it/s, loss: 0.5820 | forward: 4.51 ms | loss: 0.14 ms | backward: 1.54 ms]\n",
      "Epoch 43 Valid: 100%|██████████| 125/125 [01:13<00:00,  1.69it/s, loss=0.509]\n",
      "2025-01-14 08:14:03,522 - INFO - Epoch 43 Train mean Loss: 0.5146, Valid mean Loss: 0.5142\n",
      "2025-01-14 08:14:03,523 - INFO - Epoch 44 starts early stopping check.\n",
      "Epoch 44 Train: 100%|██████████| 1000/1000 [10:11<00:00,  1.63it/s, loss: 0.4501 | forward: 4.62 ms | loss: 0.13 ms | backward: 1.57 ms]\n",
      "Epoch 44 Valid: 100%|██████████| 125/125 [01:13<00:00,  1.70it/s, loss=0.508]\n",
      "2025-01-14 08:25:29,213 - INFO - Epoch 44 Train mean Loss: 0.5145, Valid mean Loss: 0.5139\n",
      "2025-01-14 08:25:29,213 - INFO - Epoch 45 starts early stopping check.\n",
      "Epoch 45 Train:  66%|██████▌   | 655/1000 [06:42<03:31,  1.63it/s, loss: 0.5284 | forward: 16.57 ms | loss: 0.18 ms | backward: 1.50 ms]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 322\u001b[0m\n\u001b[1;32m    320\u001b[0m loss_fn \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mBCELoss()\n\u001b[1;32m    321\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(model, train_loader, valid_loader, test_loader, optimizer, eval_step\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 322\u001b[0m best_valid \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mtrain(epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1000\u001b[39m)\n\u001b[1;32m    323\u001b[0m evaluator \u001b[38;5;241m=\u001b[39m Evaluator()\n\u001b[1;32m    324\u001b[0m result, result_str \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39meval(evaluator)\n",
      "Cell \u001b[0;32mIn[20], line 142\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, epochs, early_stopping_epochs)\u001b[0m\n\u001b[1;32m    138\u001b[0m cur_step_from_best_val \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;66;03m# train\u001b[39;00m\n\u001b[0;32m--> 142\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train_epoch(epoch_idx, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_dataloader) \u001b[38;5;66;03m# mean loss of this epoch\u001b[39;00m\n\u001b[1;32m    144\u001b[0m     \u001b[38;5;66;03m# valid\u001b[39;00m\n\u001b[1;32m    145\u001b[0m     valid_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_valid_epoch(epoch_idx, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalid_dataloader) \u001b[38;5;66;03m# mean loss of this epoch\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[20], line 248\u001b[0m, in \u001b[0;36mTrainer._train_epoch\u001b[0;34m(self, epoch_idx, train_dataloader)\u001b[0m\n\u001b[1;32m    246\u001b[0m loss_elapsed_time \u001b[38;5;241m=\u001b[39m forward_end\u001b[38;5;241m.\u001b[39melapsed_time(loss_end)\n\u001b[1;32m    247\u001b[0m backward_elapsed_time \u001b[38;5;241m=\u001b[39m loss_end\u001b[38;5;241m.\u001b[39melapsed_time(backward_end)\n\u001b[0;32m--> 248\u001b[0m pbar\u001b[38;5;241m.\u001b[39mset_postfix_str(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | forward: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mforward_elapsed_time\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m ms | loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss_elapsed_time\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m ms | backward: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbackward_elapsed_time\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m ms\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    250\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m    251\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_nan(loss)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "class BGE_FTModel(torch.nn.Module):\n",
    "    def __init__(self, rag_model):\n",
    "        super(BGE_FTModel, self).__init__()\n",
    "        logging.info(f\"Initializing Model Based on path: {rag_model}\")\n",
    "        self.text_matcher = AutoModel.from_pretrained(rag_model).to(device)\n",
    "        self.predictor = nn.Sequential(\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 1),\n",
    "            nn.Sigmoid()\n",
    "        ).to(device)\n",
    "\n",
    "        self.loss_fn = nn.BCELoss()\n",
    "        # xavier initialization for predictor\n",
    "        for m in self.predictor:\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "        \n",
    "        # logging.info(f\"Frozing Parameters...\")\n",
    "        self.frozen_target_parameters()\n",
    "        # logging.info(f\"Model Initialized.\")\n",
    "\n",
    "        self.timing_stats = defaultdict(list)  # For storing timing information\n",
    "\n",
    "    def forward(self, sample):\n",
    "        \"\"\"sample: dict like {\n",
    "            \"user_id\": list,\n",
    "            \"job_id\": list,\n",
    "            \"model_input\": {\"input_ids\": tensor, \"attention_mask\": tensor, \"token_type_ids\": tensor},\n",
    "            \"label\": tensor\n",
    "        }\n",
    "        \"\"\"\n",
    "        # Create CUDA events for timing\n",
    "        start = torch.cuda.Event(enable_timing=True)\n",
    "        step1 = torch.cuda.Event(enable_timing=True)\n",
    "        step2 = torch.cuda.Event(enable_timing=True)\n",
    "        step3 = torch.cuda.Event(enable_timing=True)\n",
    "        end = torch.cuda.Event(enable_timing=True)\n",
    "\n",
    "        start.record()\n",
    "        text_input = {k: v.squeeze(1) for k, v in sample[\"model_input\"].items()}\n",
    "        step1.record()\n",
    "        text_output = self.text_matcher(**text_input)[0][:, 0] \n",
    "        step2.record()\n",
    "        output = self.predictor(text_output)\n",
    "        step3.record()\n",
    "        end.record()\n",
    "\n",
    "        # Synchronize CUDA operations\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "        # Record timing statistics\n",
    "        self.timing_stats['data_prep'].append(start.elapsed_time(step1))\n",
    "        self.timing_stats['text_matcher'].append(step1.elapsed_time(step2))\n",
    "        self.timing_stats['predictor'].append(step2.elapsed_time(step3))\n",
    "\n",
    "        return output\n",
    "\n",
    "    \n",
    "    def calculate_loss(self, output, label):\n",
    "        #TODO: Apply more innovative loss functions.\n",
    "        return self.loss_fn(output, label)\n",
    "\n",
    "    def frozen_target_parameters(self):\n",
    "        logging.info(f\"Frozing Parameters...\")\n",
    "        for param in self.text_matcher.parameters():\n",
    "            param.requires_grad = False\n",
    "        self.print_trainable_parameters()\n",
    "    \n",
    "    def print_trainable_parameters(self):\n",
    "        total_params = sum(p.numel() for p in self.parameters())\n",
    "        trainable_params = sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "        logging.info(f\"Trainable Params: {trainable_params}. Total Params: {total_params}. Trainable Paramaters Ratio: {trainable_params/total_params}\")\n",
    "\n",
    "    def get_average_timings(self):\n",
    "        \"\"\"Get average timing statistics\"\"\"\n",
    "        return {\n",
    "            step: sum(times)/len(times) if times else 0 \n",
    "            for step, times in self.timing_stats.items()\n",
    "        }\n",
    "\n",
    "    def reset_timings(self):\n",
    "        \"\"\"Reset timing statistics\"\"\"\n",
    "        self.timing_stats.clear()\n",
    "\n",
    "# Create dataloaders with parallel processing\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=512,\n",
    "    num_workers=2,\n",
    "    shuffle=True,\n",
    "    collate_fn=lambda x: collate_fn(x, tokenizer),\n",
    "    pin_memory=True,\n",
    "    persistent_workers=True\n",
    ")\n",
    "\n",
    "valid_loader = DataLoader(\n",
    "    valid_dataset,\n",
    "    batch_size=512,\n",
    "    num_workers=2,\n",
    "    shuffle=False,\n",
    "    collate_fn=lambda x: collate_fn(x, tokenizer),\n",
    "    pin_memory=True,\n",
    "    persistent_workers=True\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=512,\n",
    "    num_workers=2,\n",
    "    shuffle=False,\n",
    "    collate_fn=lambda x: collate_fn(x, tokenizer),\n",
    "    pin_memory=True,\n",
    "    persistent_workers=True\n",
    ")\n",
    "\n",
    "class Trainer(object):\n",
    "    def __init__(self, model, train_dataloader, valid_dataloader, test_dataloader, optimizer, eval_step, verbose=True):\n",
    "        self.model = model\n",
    "        self.train_dataloader = train_dataloader\n",
    "        self.valid_dataloader = valid_dataloader\n",
    "        self.test_dataloader = test_dataloader\n",
    "        self.clip_grad_norm = None\n",
    "        self.optimizer = optimizer\n",
    "        self.eval_step = 1\n",
    "\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def train(self, epochs, early_stopping_epochs=5):\n",
    "        train_loss = valid_loss = float('inf')\n",
    "        \n",
    "        # the below init values are for early stopping\n",
    "        best_valid = cur_best_valid = float('inf')\n",
    "        cur_step_from_best_val = 0\n",
    "        \n",
    "        for epoch_idx in range(epochs):\n",
    "            # train\n",
    "            train_loss = self._train_epoch(epoch_idx, self.train_dataloader) # mean loss of this epoch\n",
    "\n",
    "            # valid\n",
    "            valid_loss = self._valid_epoch(epoch_idx, self.valid_dataloader) # mean loss of this epoch\n",
    "            if self.verbose:\n",
    "                logging.info(f\"Epoch {epoch_idx} Train mean Loss: {train_loss:.4f}, Valid mean Loss: {valid_loss:.4f}\")\n",
    "                \n",
    "            # early stopping\n",
    "            if (epoch_idx + 1) % self.eval_step == 0:\n",
    "                if self.verbose:\n",
    "                    logging.info(f\"Epoch {epoch_idx + 1} starts early stopping check.\")\n",
    "                cur_best_valid, cur_step_from_best_val, stop_flag, update_flag = self._early_stopping(\n",
    "                    valid_loss, cur_best_valid, cur_step_from_best_val, early_stopping_epochs, lower_is_better=True) # -> best, cur_step, stop_flag, update_flag\n",
    "                \n",
    "                if update_flag:\n",
    "                    best_valid = cur_best_valid\n",
    "            \n",
    "                if stop_flag:\n",
    "                    if self.verbose:\n",
    "                        logging.info(f\"Early stopping at epoch {epoch_idx}\")\n",
    "                    break\n",
    "        \n",
    "        return best_valid\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def eval(self, evaluator):\n",
    "        \"\"\"\n",
    "        Using the test dataloader to evaluate the model.\n",
    "        For each (cv, jd) pair, we predict the probability of the cv being browsed.\n",
    "        The evaluation results are saved to the save_path as the following format:\n",
    "\n",
    "        The evaluation matriceare all based on top-k selection. for each cv_i, the \n",
    "        top-k are selected from all (cv_i, jd) pairs that appear in the test set. \n",
    "        After consideration, due to the context of precise-recommendation matching \n",
    "        task, we decide if jd_j are in the testset records but not being recorded \n",
    "        with cv_i in the testset, we will not consider jd_j in the top-k selection\n",
    "        for cv_i.\n",
    "        \n",
    "        params:\n",
    "            evaluator: Evaluator, the evaluator for the model.\n",
    "\n",
    "        return:\n",
    "            result: list\n",
    "        \"\"\"\n",
    "        # set model to eval mode\n",
    "        if self.verbose:\n",
    "            logging.info(\"Start evaluating on test set\")\n",
    "        self.model.eval()\n",
    "\n",
    "\n",
    "        pbar = tqdm(enumerate(self.test_dataloader), total=len(self.test_dataloader), desc=\"Matrices Evaluation     \")\n",
    "       \n",
    "        # predicting scores, while saving the predictions records.\n",
    "        for step, sample in pbar:\n",
    "            uid = sample[\"user_id\"] # List of length bs\n",
    "            sample = dict2device(sample, device) # {\"model_input_jd\": dict, \"model_input_cv\": dict, \"label\": tensor}\n",
    "            scores = self.model(sample).squeeze(-1).cpu().tolist()\n",
    "            labels = sample[\"label\"].squeeze(-1).cpu().tolist()\n",
    "            evaluator.collect(uid, scores, labels)\n",
    "\n",
    "        # evaluate the results\n",
    "        results, results_str = evaluator.evaluate([1, 5, 10])\n",
    "        return results, results_str\n",
    "\n",
    "    # below is indirect copy from https://github.com/hyp1231/SHPJF/tree/master/model\n",
    "    def _train_epoch(self, epoch_idx: int, train_dataloader: DataLoader):\n",
    "        self.model.train()\n",
    "        total_loss = 0\n",
    "        total_batches = len(train_dataloader)\n",
    "        \n",
    "        pbar = tqdm(enumerate(train_dataloader), total=total_batches, desc=f\"Epoch {epoch_idx} Train\")\n",
    "\n",
    "        for step, sample in pbar:\n",
    "            # Create new events for each iteration\n",
    "            start = torch.cuda.Event(enable_timing=True)\n",
    "            forward_end = torch.cuda.Event(enable_timing=True)\n",
    "            loss_end = torch.cuda.Event(enable_timing=True)\n",
    "            backward_end = torch.cuda.Event(enable_timing=True)\n",
    "\n",
    "            start.record()\n",
    "            sample = dict2device(sample, device)\n",
    "            label = sample[\"label\"].unsqueeze(1).to(device)\n",
    "            self.optimizer.zero_grad()\n",
    "            \n",
    "            # 模型前向传播\n",
    "            output = self.model(sample)\n",
    "            forward_end.record()\n",
    "            # 损失计算\n",
    "            loss = self.model.calculate_loss(output, label)\n",
    "            loss_end.record()\n",
    "            # 反向传播\n",
    "            loss.backward()\n",
    "            backward_end.record()\n",
    "            \n",
    "            # 梯度裁剪（如果启用）\n",
    "            if self.clip_grad_norm:\n",
    "                clip_grad_norm_(self.model.parameters(), **self.clip_grad_norm)\n",
    "            \n",
    "            # 优化器步骤\n",
    "            self.optimizer.step()\n",
    "            torch.cuda.synchronize()\n",
    "\n",
    "            # 更新进度条显示\n",
    "            forward_elapsed_time = start.elapsed_time(forward_end)\n",
    "            loss_elapsed_time = forward_end.elapsed_time(loss_end)\n",
    "            backward_elapsed_time = loss_end.elapsed_time(backward_end)\n",
    "            pbar.set_postfix_str(f\"loss: {loss.item():.4f} | forward: {forward_elapsed_time:.2f} ms | loss: {loss_elapsed_time:.2f} ms | backward: {backward_elapsed_time:.2f} ms\")\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            self._check_nan(loss)\n",
    "            \n",
    "        return total_loss / total_batches\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def _valid_epoch(self, epoch_idx: int, valid_dataloader: DataLoader):\n",
    "        \"\"\"valid the model with valid data by calculate the loss\n",
    "        \"\"\"\n",
    "        \n",
    "        # set model to eval mode\n",
    "        self.model.eval()\n",
    "        total_loss = 0\n",
    "        total_batches = len(valid_dataloader)\n",
    "        pbar = tqdm(enumerate(valid_dataloader), total=total_batches, desc=f\"Epoch {epoch_idx} Valid\")\n",
    "\n",
    "        # calculate loss on validation set\n",
    "        for step, sample in pbar:\n",
    "            sample = dict2device(sample, device) # batch: {\"model_input_jd\": dict, \"model_input_cv\": dict, \"label\": tensor}\n",
    "            label = sample[\"label\"].unsqueeze(1) # (bs, 1)\n",
    "            output = self.model(sample) # (bs, 1)\n",
    "            loss = self.model.calculate_loss(output, label) # output: (bs, 1), label: (bs, 1)\n",
    "            pbar.set_postfix(loss=loss.item())\n",
    "            total_loss += loss.item()\n",
    "            self._check_nan(loss)\n",
    "\n",
    "        return total_loss / total_batches\n",
    "    \n",
    "    def _early_stopping(self, value, best, cur_step, max_step, lower_is_better=True):\n",
    "        \"\"\"validation-based early stopping\n",
    "\n",
    "        Args:\n",
    "            value (float): current result\n",
    "            best (float): best result\n",
    "            cur_step (int): the number of consecutive steps that did not exceed the best result\n",
    "            max_step (int): threshold steps for stopping\n",
    "\n",
    "        Returns:\n",
    "            tuple:\n",
    "            - best: float,\n",
    "            best result after this step\n",
    "            - cur_step: int,\n",
    "            the number of consecutive steps that did not exceed the best result after this step\n",
    "            - stop_flag: bool,\n",
    "            whether to stop\n",
    "            - update_flag: bool,\n",
    "            whether to update\n",
    "        \"\"\"\n",
    "        stop_flag = False\n",
    "        update_flag = False\n",
    "\n",
    "        better = value < best if lower_is_better else value > best\n",
    "        if better:\n",
    "            cur_step = 0\n",
    "            best = value\n",
    "            update_flag = True\n",
    "        else:\n",
    "            cur_step += 1\n",
    "            if cur_step > max_step:\n",
    "                stop_flag = True\n",
    "        return best, cur_step, stop_flag, update_flag\n",
    "\n",
    "    def _check_nan(self, loss):\n",
    "        if torch.isnan(loss).any():\n",
    "            raise ValueError(\"Model diverged with loss = NaN\")\n",
    "        return\n",
    "\n",
    "    \n",
    "model = BGE_FTModel(bge_path).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "loss_fn = nn.BCELoss()\n",
    "trainer = Trainer(model, train_loader, valid_loader, test_loader, optimizer, eval_step=1)\n",
    "best_valid = trainer.train(epochs = 1000)\n",
    "evaluator = Evaluator()\n",
    "result, result_str = trainer.eval(evaluator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'auc': 0.52,\n",
       " 'logloss': 0.8871039078754641,\n",
       " 'ndcg@1': 0.20774647887323944,\n",
       " 'ndcg@5': 0.2208454705797289,\n",
       " 'ndcg@10': 0.2208454705797289,\n",
       " 'precision@1': 0.20774647887323944,\n",
       " 'precision@5': 0.20657276995305165,\n",
       " 'precision@10': 0.20657276995305165,\n",
       " 'recall@1': 0.8961538461538462,\n",
       " 'recall@5': 1.0,\n",
       " 'recall@10': 1.0,\n",
       " 'map@1': 0.20774647887323944,\n",
       " 'map@5': 0.21801643192488263,\n",
       " 'map@10': 0.21801643192488263,\n",
       " 'mrr@1': 0.21801643192488263,\n",
       " 'mrr@5': 0.21801643192488263,\n",
       " 'mrr@10': 0.21801643192488263}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'BGE_FTDataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[0;32m----> 5\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m BGE_FTDataset(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdataset/processed_train.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, tokenizer,ratio\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      6\u001b[0m valid_dataset \u001b[38;5;241m=\u001b[39m BGE_FTDataset(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalid\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdataset/processed_valid.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, tokenizer,ratio\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      7\u001b[0m test_dataset \u001b[38;5;241m=\u001b[39m BGE_FTDataset(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdataset/processed_test.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, tokenizer, ratio\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'BGE_FTDataset' is not defined"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "train_dataset = BGE_FTDataset('train', 'dataset/processed_train.csv', tokenizer,ratio=1)\n",
    "valid_dataset = BGE_FTDataset('valid', \"dataset/processed_valid.csv\", tokenizer,ratio=1)\n",
    "test_dataset = BGE_FTDataset('test', \"dataset/processed_test.csv\", tokenizer, ratio=1)\n",
    "\n",
    "def save_tokenized_dataset_to_json(dataset, output_file):\n",
    "    \"\"\"\n",
    "    将 IterableDataset 的 tokenized 文本逐条存储到 JSON 文件中。\n",
    "\n",
    "    Args:\n",
    "        dataset (IterableDataset): 实现了 __iter__ 方法的 dataset，如 BGE_FTDataset。\n",
    "        output_file (str): 要保存的 JSON 文件路径。\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        for sample in tqdm(dataset, desc=\"Saving tokenized dataset to JSON\"):\n",
    "            # 将 PyTorch 张量转换为可序列化的格式\n",
    "            serialized_sample = {\n",
    "                \"user_id\": sample[\"user_id\"],  # 字符串或基本数据类型，直接存储\n",
    "                \"job_id\": sample[\"job_id\"],    # 字符串或基本数据类型，直接存储\n",
    "                \"model_input\": {\n",
    "                    \"model_input_cv\": {\n",
    "                        k: v.tolist() for k, v in sample[\"model_input\"][\"model_input_cv\"].items()\n",
    "                    },\n",
    "                    \"model_input_jd\": {\n",
    "                        k: v.tolist() for k, v in sample[\"model_input\"][\"model_input_jd\"].items()\n",
    "                    },\n",
    "                    \"label\": sample[\"model_input\"][\"label\"].item()  # 转为 float\n",
    "                }\n",
    "            }\n",
    "            # 将该样本写入 JSON 文件，每行一个样本\n",
    "            f.write(json.dumps(serialized_sample) + '\\n')\n",
    "\n",
    "train_dataset = BGE_FTDataset('train', 'dataset/processed_train.csv', tokenizer,ratio=1)\n",
    "valid_dataset = BGE_FTDataset('valid', \"dataset/processed_valid.csv\", tokenizer,ratio=1)\n",
    "test_dataset = BGE_FTDataset('test', \"dataset/processed_test.csv\", tokenizer, ratio=1)\n",
    "\n",
    "save_tokenized_dataset_to_json(train_dataset, '/media/wuyuhuan/tokenids/train_token_jsons.json')\n",
    "save_tokenized_dataset_to_json(valid_dataset, '/media/wuyuhuan/tokenids/valid_token_jsons.json')\n",
    "save_tokenized_dataset_to_json(test_dataset, '/media/wuyuhuan/tokenids/test_token_jsons.json')\n",
    "# 示例用法\n",
    "# 假设你已经定义了 BGE_FTDataset\n",
    "# dataset = BGE_FTDataset(mode='train', in_file='path/to/input.csv', tokenizer=your_tokenizer)\n",
    "# save_tokenized_dataset_to_json(dataset, output_file='output.json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'auc': 0.5313714494507185,\n",
       " 'logloss': 0.5764790056676369,\n",
       " 'ndcg@1': 0.2520190023752969,\n",
       " 'ndcg@5': 0.4664345684607033,\n",
       " 'ndcg@10': 0.5028075114044772,\n",
       " 'precision@1': 0.2520190023752969,\n",
       " 'precision@5': 0.24563737133808594,\n",
       " 'precision@10': 0.2415688270557644,\n",
       " 'recall@1': 0.13997698901130962,\n",
       " 'recall@5': 0.4863068289489413,\n",
       " 'recall@10': 0.7198235059496314,\n",
       " 'map@1': 0.2520190023752969,\n",
       " 'map@5': 0.38942564001055857,\n",
       " 'map@10': 0.38262279548927025,\n",
       " 'mrr@1': 0.4238476160112081,\n",
       " 'mrr@5': 0.4238476160112081,\n",
       " 'mrr@10': 0.4238476160112081}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "recbole",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
