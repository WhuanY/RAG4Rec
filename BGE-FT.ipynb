{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    ".inter\n",
    "(g, j, s)\n",
    "g.info <- .udoc\n",
    "j.info <- .idoc\n",
    "\n",
    "g.strRep -> C \n",
    "            o          M\n",
    "            n --BGE--> L ---> out\n",
    "            c --BGE--> P ---> put\n",
    "            a          s\n",
    "j.strRep -> t\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mThe kernel died. Error: ... View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "bge_path = \"/media/wuyuhuan/bge-small-zh\"\n",
    "from torch.utils.data import Dataset, IterableDataset\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.clip_grad import clip_grad_norm_\n",
    "\n",
    "import logging\n",
    "\n",
    "import time\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "logging.info(f\"Usable GPU: {torch.cuda.device_count()}\")  \n",
    "tokenizer = AutoTokenizer.from_pretrained(bge_path)\n",
    "\n",
    "def same_seed(seed):\n",
    "    '''Fixes random number generator seeds for reproducibility.'''\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def dict2device(data, device):\n",
    "    for k, v in data.items():\n",
    "        if isinstance(v, dict):\n",
    "            data[k] = dict2device(v, device)\n",
    "        elif isinstance(v, torch.Tensor):\n",
    "            data[k] = v.to(device)\n",
    "    return data\n",
    "\n",
    "same_seed(42)\n",
    "\n",
    "class BGE_FTDataset(Dataset):\n",
    "    def __init__(self, mode: str, in_file: str, tokenizer: AutoTokenizer, ratio: float = 1.0):\n",
    "        \"\"\"\n",
    "        mode: str, one of ['train', 'valid', 'test']\n",
    "        in_file: str, path to the input csv file.\n",
    "        tokenizer: AutoTokenizer, tokenizer for the model.\n",
    "        ratio: float, the ratio of the data to be used. Default: 1.0. \n",
    "            set to 0.01 for functionality testing.\n",
    "        \"\"\"\n",
    "        self.mode = mode\n",
    "        self.dataset = pd.read_csv(in_file).sample(frac=ratio)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(bge_path)\n",
    "        logging.info(f\"Dataset {mode} Loaded. Shape: {self.dataset.shape}\")\n",
    "\n",
    "        self.data = []\n",
    "        self.tokenize_data() # -> self.data will look like: [(user_id, job_id, tokenized_cv, tokenized_jd, label), ...] \n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.dataset.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"user_id\": self.data[idx][0],\n",
    "            \"job_id\": self.data[idx][1],\n",
    "            \"model_input\": {\n",
    "                \"model_input_cv\": self.data[idx][2],\n",
    "                \"model_input_jd\": self.data[idx][3],\n",
    "                \"label\": self.data[idx][4],\n",
    "            }\n",
    "        }\n",
    "        \n",
    "    def tokenize_data(self, process_batch_size=10000):\n",
    "        \"\"\"\n",
    "        Tokenize the data in the dataset. The result will be stored in self.data.\n",
    "        \"\"\"\n",
    "        logging.info(f\"Tokenizing {self.mode} Dataset\")\n",
    "        \n",
    "        iter_data = tqdm(\n",
    "            range(0, len(self.dataset), process_batch_size),\n",
    "            total= len(self.dataset) // process_batch_size + 1,\n",
    "            desc=f\"Tokenizing {self.mode} Dataset\"\n",
    "        )\n",
    "        for i in iter_data:\n",
    "            batch = self.dataset.iloc[i:i+process_batch_size]\n",
    "\n",
    "            tokenized_cv = self.tokenizer(batch['cv'].tolist(),padding='max_length',truncation=True,return_tensors='pt') # tokenized_cv: {input_ids: torch.tensor(process_batch_size, d), attention_mask: (process_batch_size, d), token_type_ids: (process_batch_size, d)}\n",
    "\n",
    "            tokenized_jd = self.tokenizer(\n",
    "                batch['jd'].tolist(),padding='max_length',truncation=True,return_tensors='pt') # tokenized_jd: {input_ids: torch.tensor(process_batch_size, d), attention_mask: (process_batch_size, d), token_type_ids: (process_batch_size, d)}\n",
    "\n",
    "            \n",
    "            self.data.extend([(\n",
    "                user_id, \n",
    "                job_id, \n",
    "                {\"input_ids\": tokenized_cv['input_ids'][i], \"attention_mask\": tokenized_cv['attention_mask'][i], \"token_type_ids\": tokenized_cv['token_type_ids'][i]},\n",
    "                {\"input_ids\": tokenized_jd['input_ids'][i], \"attention_mask\": tokenized_jd['attention_mask'][i], \"token_type_ids\": tokenized_jd['token_type_ids'][i]},\n",
    "                torch.tensor(label, dtype=torch.float32)\n",
    "            )\n",
    "            for i, (user_id, job_id, label) in enumerate(\n",
    "                zip(batch['user_id:token'].to_numpy(), \n",
    "                    batch['job_id:token'].to_numpy(), \n",
    "                    batch['browsed:label'].to_numpy()))])\n",
    "\n",
    "train_dataset = BGE_FTDataset('train', 'dataset/processed_train.csv', tokenizer,ratio=1)\n",
    "valid_dataset = BGE_FTDataset('valid', \"dataset/processed_valid.csv\", tokenizer,ratio=1)\n",
    "test_dataset = BGE_FTDataset('test', \"dataset/processed_test.csv\", tokenizer, ratio=1)\n",
    "\n",
    "# train_dataset[0]\n",
    "# valid_dataset[0]\n",
    "# test_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'user_id': '2c32e5860f69b648084ff0b8dd2bad2d',\n",
       " 'job_id': '6fcada98d14321e05be0783ee6fc6152',\n",
       " 'model_input': {'model_input_cv': {'input_ids': tensor([ 101,  702,  782, 5042, 1325,  131,  523, 2339,  868, 5307, 1325,  524,\n",
       "           7481, 6407,  170, 1486, 6418, 2360,  170, 1962, 1936, 2552,  170, 1690,\n",
       "           3462,  170, 7566, 2193,  170, 6598, 3160,  170, 2145, 3302,  683, 1447,\n",
       "            170, 5381, 5317,  170,  712, 1220, 2595,  170, 1726, 6393,  170, 2970,\n",
       "           2521,  170,  772, 1501,  170, 6381, 2497,  170, 3119, 1355,  170,  833,\n",
       "           1447,  170,  924,  934,  170, 6844, 2418, 5543, 1213,  170, 2970, 1420,\n",
       "           4510, 6413,  170, 6934,  816,  170, 4633, 6381,  170, 7218, 1545,  170,\n",
       "           2145, 2787,  170, 1310, 4495,  170, 5335, 2844,  523, 3309, 3307, 6121,\n",
       "            689,  524,  131, 3136, 5509,  120, 1824, 6378,  120, 7368, 3413,  117,\n",
       "           2031,  727,  120,  860, 5509,  120,  828, 7312,  523, 2496, 1184, 6121,\n",
       "            689,  524, 1278, 4545,  120, 2844, 4415,  120, 5401, 2159,  120,  924,\n",
       "            978,  120, 1310, 4495, 3302, 1218,  102,    0,    0,    0,    0,    0,\n",
       "              0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "              0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "              0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "              0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "              0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "              0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "              0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "              0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "              0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "              0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "              0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "              0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "              0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "              0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "              0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "              0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "              0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "              0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "              0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "              0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "              0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "              0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "              0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "              0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "              0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "              0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "              0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "              0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "              0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "              0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "              0,    0,    0,    0,    0,    0,    0,    0]),\n",
       "   'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0]),\n",
       "   'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0])},\n",
       "  'model_input_jd': {'input_ids': tensor([101, 139, 100, 102,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "             0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "             0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "             0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "             0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "             0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "             0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "             0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "             0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "             0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "             0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "             0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "             0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "             0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "             0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "             0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "             0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "             0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "             0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "             0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "             0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "             0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "             0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "             0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "             0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "             0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "             0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "             0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "             0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "             0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "             0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "             0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "             0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "             0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "             0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "             0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "             0,   0,   0,   0,   0,   0,   0,   0]),\n",
       "   'attention_mask': tensor([1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0]),\n",
       "   'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0])},\n",
       "  'label': tensor(0.)}}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from sklearn.metrics import f1_score, accuracy_score, roc_auc_score, log_loss\n",
    "class Evaluator:\n",
    "    \"\"\"\n",
    "    Evaluator for the BGE-FT model.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        #TODO: the label values are currently only assumed to be binary\n",
    "        # For further experiments, we need to make the label values more general.  \n",
    "        self.uid2topk = {} # {uid: [(score, label), ...]}  \n",
    "        \n",
    "        self.topk = 10\n",
    "        self.metric2func = {\n",
    "            \"ndcg\": self._ndcg,\n",
    "            \"precision\": self._precision,\n",
    "            \"recall\": self._recall,\n",
    "            \"map\": self._map,\n",
    "            \"mrr\": self._mrr,\n",
    "            \"auc\": self._auc,\n",
    "            \"logloss\": self._logloss,\n",
    "        }\n",
    "        self.cls_metrics = [\"auc\", \"logloss\"]\n",
    "        self.rkg_metrics = [\"ndcg\", \"precision\", \"recall\", \"map\", \"mrr\"]\n",
    "        \n",
    "\n",
    "    def collect(self, uid, score, label):\n",
    "        \"\"\"\n",
    "        Process a batch of data. Save the data to the evaluator. \n",
    "        Input params are lists of same length as batch size.\n",
    "        After this func, uid2topk will look like: {uid: [(score, label), ...]}\n",
    "        where each uid has interaction list sorted by score\n",
    "\n",
    "        Args:\n",
    "            uid: list, list of user ids.  \n",
    "            score: list, list of scores.\n",
    "            label: list, list of labels.\n",
    "            \n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        for u, s, l in zip(uid, score, label):\n",
    "            if u not in self.uid2topk:\n",
    "                self.uid2topk[u] = []\n",
    "            self.uid2topk[u].append((s, l)) \n",
    "\n",
    "        for u in self.uid2topk:\n",
    "            self.uid2topk[u] = sorted(self.uid2topk[u], key=lambda x: x[0], reverse=True)\n",
    "         \n",
    "    def evaluate(self, K: List[int]):\n",
    "        \"\"\"\n",
    "        Evaluate the model using the collected data and the pass value k.\n",
    "        Args:\n",
    "            K: List[int], a list of k values for ranking metrics.\n",
    "        \n",
    "        return:\n",
    "            result: dict, a dictionary of evaluation results.\n",
    "            result_str: str, a formatted string of the evaluation results.\n",
    "        \"\"\"\n",
    "        result = {} # {cls_m1: value1, cls_m2: value2, ..., rkg_m1@k1: value1, rkg_m2@k2: value2, ...}\n",
    "\n",
    "        # Calculate the metrics\n",
    "        for cls_metric in self.cls_metrics:\n",
    "            matric_val = self.metric2func[cls_metric]()\n",
    "            result[cls_metric] = matric_val\n",
    "\n",
    "        for rkg_metric in self.rkg_metrics:\n",
    "            for k in K:\n",
    "                result[rkg_metric + '@' + str(k)] = self.metric2func[rkg_metric](k)\n",
    "        \n",
    "        result_str = self._format_str(result)\n",
    "        return result, result_str\n",
    "    \n",
    "\n",
    "    # below are the ranking metric functions. With most of are indirect copy from the recbole.metrics.\n",
    "    def _ndcg(self, k):\n",
    "        base = []\n",
    "        idcg = []\n",
    "\n",
    "        # save base and idcg(Ideal DCG) for each position\n",
    "        for i in range(k):\n",
    "            base.append(np.log(2) / np.log(i + 2)) # np.log(2) / np.log(i + 2) = log_{i + 2}(2)\n",
    "            if i > 0:\n",
    "                idcg.append(base[i] + idcg[i - 1])\n",
    "            else:\n",
    "                idcg.append(base[i])\n",
    "\n",
    "        # calculate the dcg\n",
    "        tot = 0\n",
    "        for uid in self.uid2topk:\n",
    "            dcg = 0\n",
    "            pos = 0\n",
    "            for i, (score, label) in enumerate(self.uid2topk[uid][:k]):\n",
    "                dcg += (2 ** label - 1) * base[i] # 2^rel - 1 / log_(2)(i + 1)\n",
    "                pos += label # TODO: If label is not binary, this should be modified.\n",
    "            tot += dcg / idcg[int(pos) - 1]\n",
    "        return tot / len(self.uid2topk)\n",
    "\n",
    "    def _precision(self, k):\n",
    "        tot = 0\n",
    "        valid_length = 0\n",
    "        for uid in self.uid2topk:\n",
    "            rec = 0\n",
    "            rel = 0\n",
    "            for i, (score, label) in enumerate(self.uid2topk[uid][:k]):\n",
    "                rec += 1\n",
    "                rel += label # TODO: If label is not binary, this should (maybe) be modified.\n",
    "            try:\n",
    "                tot += rel / rec\n",
    "                valid_length += 1\n",
    "            except:\n",
    "                continue\n",
    "        return tot / valid_length\n",
    "    \n",
    "    def _recall(self, k):\n",
    "        tot = 0\n",
    "        valid_length = 0\n",
    "        for uid in self.uid2topk:\n",
    "            rec = 0\n",
    "            rel = 0\n",
    "            for i, (score, label) in enumerate(self.uid2topk[uid]):\n",
    "                if i < k:\n",
    "                    rec += label\n",
    "                rel += label #TODO: If label is not binary, this should (maybe) be modified.\n",
    "            try:\n",
    "                tot += rec / rel\n",
    "                valid_length += 1\n",
    "            except:\n",
    "                continue\n",
    "        return tot / valid_length\n",
    "\n",
    "    # TODO: The MAP and MRR functions are not understood yet.\n",
    "    def _map(self,k):\n",
    "        tot = 0\n",
    "        for uid in self.uid2topk:\n",
    "            tp = 0\n",
    "            pos = 0\n",
    "            ap = 0\n",
    "            for i, (score, label) in enumerate(self.uid2topk[uid][:k]):\n",
    "                if label == 1:\n",
    "                    tp += 1\n",
    "                    pos += 1\n",
    "                    ap += tp / (i + 1)\n",
    "            if pos > 0:\n",
    "                tot += ap / pos\n",
    "        return tot / len(self.uid2topk)\n",
    "\n",
    "    def _mrr(self, k):\n",
    "        tot = 0\n",
    "        for uid in self.uid2topk:\n",
    "            for i, (score, label) in enumerate(self.uid2topk[uid]):\n",
    "                if label == 1:\n",
    "                    tot += 1 / (i + 1)\n",
    "                    break\n",
    "        return tot / len(self.uid2topk)\n",
    "        \n",
    "    \n",
    "    # below are the classification metric functions\n",
    "    def _auc(self):\n",
    "        \"\"\"\n",
    "        Calculate the AUC score.\n",
    "        \"\"\"\n",
    "        total_auc = 0\n",
    "        valid_auc_num = 0\n",
    "        for uid, topk in self.uid2topk.items():\n",
    "            score, labels = zip(*topk)\n",
    "            try:\n",
    "                auc = roc_auc_score(labels, score)\n",
    "                total_auc += auc\n",
    "                valid_auc_num += 1\n",
    "            except:\n",
    "                continue\n",
    "        return total_auc / valid_auc_num\n",
    "        \n",
    "\n",
    "    def _logloss(self):\n",
    "        \"\"\"\n",
    "        Calculate the logloss.\n",
    "        \"\"\"\n",
    "        total_logloss = 0\n",
    "        valid_logloss_num = 0\n",
    "        for uid, topk in self.uid2topk.items():\n",
    "            score, labels = zip(*topk)\n",
    "            try:\n",
    "                logloss = log_loss(labels, score)\n",
    "                valid_logloss_num += 1\n",
    "                total_logloss += logloss\n",
    "            except:\n",
    "                continue\n",
    "        return total_logloss / valid_logloss_num\n",
    "\n",
    "    # other utility functions for evaluator\n",
    "    def _format_str(self, result):\n",
    "        res = ''\n",
    "        for metric in result.keys():\n",
    "            res += '\\n\\t{}:\\t{:.4f}'.format(metric, result[metric])\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-07 00:41:39,780 - INFO - Initializing Model Based on path: /media/wuyuhuan/bge-small-zh\n",
      "2025-01-07 00:41:42,277 - INFO - Frozing Parameters...\n",
      "2025-01-07 00:41:42,283 - INFO - Model Initialized.\n",
      "2025-01-07 00:41:42,297 - INFO - Trainable Params: 266529. Total Params: 48174369. Trainable Paramaters Ratio: 0.005532589331891405\n",
      "Epoch 0 Train: 100%|██████████| 1999/1999 [44:18<00:00,  1.33s/it, loss=0.367]\n",
      "Epoch 0 Valid: 100%|██████████| 250/250 [05:22<00:00,  1.29s/it, loss=0.49] \n",
      "2025-01-07 01:31:23,462 - INFO - Epoch 0 Train mean Loss: 0.5220, Valid mean Loss: 0.5172\n",
      "Epoch 1 Train: 100%|██████████| 1999/1999 [44:03<00:00,  1.32s/it, loss=0.357]\n",
      "Epoch 1 Valid: 100%|██████████| 250/250 [05:19<00:00,  1.28s/it, loss=0.489]\n",
      "2025-01-07 02:20:47,403 - INFO - Epoch 1 Train mean Loss: 0.5200, Valid mean Loss: 0.5167\n",
      "2025-01-07 02:20:47,407 - INFO - Epoch 2 starts early stopping check.\n",
      "Epoch 2 Train: 100%|██████████| 1999/1999 [43:52<00:00,  1.32s/it, loss=0.351]\n",
      "Epoch 2 Valid: 100%|██████████| 250/250 [05:21<00:00,  1.29s/it, loss=0.489]\n",
      "2025-01-07 03:10:01,781 - INFO - Epoch 2 Train mean Loss: 0.5195, Valid mean Loss: 0.5162\n",
      "Epoch 3 Train: 100%|██████████| 1999/1999 [43:43<00:00,  1.31s/it, loss=0.352]\n",
      "Epoch 3 Valid: 100%|██████████| 250/250 [05:27<00:00,  1.31s/it, loss=0.49] \n",
      "2025-01-07 03:59:13,119 - INFO - Epoch 3 Train mean Loss: 0.5189, Valid mean Loss: 0.5158\n",
      "2025-01-07 03:59:13,123 - INFO - Epoch 4 starts early stopping check.\n",
      "Epoch 4 Train: 100%|██████████| 1999/1999 [43:37<00:00,  1.31s/it, loss=0.348]\n",
      "Epoch 4 Valid: 100%|██████████| 250/250 [05:19<00:00,  1.28s/it, loss=0.489]\n",
      "2025-01-07 04:48:09,915 - INFO - Epoch 4 Train mean Loss: 0.5185, Valid mean Loss: 0.5154\n",
      "Epoch 5 Train: 100%|██████████| 1999/1999 [43:30<00:00,  1.31s/it, loss=0.352]\n",
      "Epoch 5 Valid: 100%|██████████| 250/250 [05:20<00:00,  1.28s/it, loss=0.49] \n",
      "2025-01-07 05:37:00,859 - INFO - Epoch 5 Train mean Loss: 0.5181, Valid mean Loss: 0.5150\n",
      "2025-01-07 05:37:00,862 - INFO - Epoch 6 starts early stopping check.\n",
      "Epoch 6 Train: 100%|██████████| 1999/1999 [43:30<00:00,  1.31s/it, loss=0.336]\n",
      "Epoch 6 Valid: 100%|██████████| 250/250 [05:19<00:00,  1.28s/it, loss=0.489]\n",
      "2025-01-07 06:25:50,963 - INFO - Epoch 6 Train mean Loss: 0.5175, Valid mean Loss: 0.5146\n",
      "Epoch 7 Train: 100%|██████████| 1999/1999 [43:29<00:00,  1.31s/it, loss=0.338]\n",
      "Epoch 7 Valid: 100%|██████████| 250/250 [05:20<00:00,  1.28s/it, loss=0.488]\n",
      "2025-01-07 07:14:41,223 - INFO - Epoch 7 Train mean Loss: 0.5171, Valid mean Loss: 0.5145\n",
      "2025-01-07 07:14:41,226 - INFO - Epoch 8 starts early stopping check.\n",
      "Epoch 8 Train: 100%|██████████| 1999/1999 [43:37<00:00,  1.31s/it, loss=0.336]\n",
      "Epoch 8 Valid: 100%|██████████| 250/250 [05:19<00:00,  1.28s/it, loss=0.49] \n",
      "2025-01-07 08:03:37,453 - INFO - Epoch 8 Train mean Loss: 0.5165, Valid mean Loss: 0.5140\n",
      "Epoch 9 Train: 100%|██████████| 1999/1999 [43:45<00:00,  1.31s/it, loss=0.33] \n",
      "Epoch 9 Valid: 100%|██████████| 250/250 [05:23<00:00,  1.29s/it, loss=0.49] \n",
      "2025-01-07 08:52:46,420 - INFO - Epoch 9 Train mean Loss: 0.5159, Valid mean Loss: 0.5135\n",
      "2025-01-07 08:52:46,423 - INFO - Epoch 10 starts early stopping check.\n",
      "Epoch 10 Train: 100%|██████████| 1999/1999 [44:17<00:00,  1.33s/it, loss=0.324]\n",
      "Epoch 10 Valid: 100%|██████████| 250/250 [05:23<00:00,  1.29s/it, loss=0.49] \n",
      "2025-01-07 09:42:27,665 - INFO - Epoch 10 Train mean Loss: 0.5155, Valid mean Loss: 0.5135\n",
      "Epoch 11 Train: 100%|██████████| 1999/1999 [44:02<00:00,  1.32s/it, loss=0.332]\n",
      "Epoch 11 Valid: 100%|██████████| 250/250 [05:23<00:00,  1.29s/it, loss=0.494]\n",
      "2025-01-07 10:31:53,299 - INFO - Epoch 11 Train mean Loss: 0.5149, Valid mean Loss: 0.5130\n",
      "2025-01-07 10:31:53,304 - INFO - Epoch 12 starts early stopping check.\n",
      "Epoch 12 Train:   5%|▍         | 95/1999 [02:07<42:42,  1.35s/it, loss=0.511]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 267\u001b[0m\n\u001b[1;32m    265\u001b[0m loss_fn \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mBCELoss()\n\u001b[1;32m    266\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(model, train_loader, valid_loader, test_loader, optimizer, eval_step\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 267\u001b[0m best_valid \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mtrain(epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1000\u001b[39m)\n\u001b[1;32m    268\u001b[0m evaluator \u001b[38;5;241m=\u001b[39m Evaluator()\n\u001b[1;32m    269\u001b[0m result, result_str \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39meval(evaluator)\n",
      "Cell \u001b[0;32mIn[33], line 100\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, epochs, early_stopping_epochs)\u001b[0m\n\u001b[1;32m     96\u001b[0m cur_step_from_best_val \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[1;32m     99\u001b[0m     \u001b[38;5;66;03m# train\u001b[39;00m\n\u001b[0;32m--> 100\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train_epoch(epoch_idx, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_dataloader) \u001b[38;5;66;03m# mean loss of this epoch\u001b[39;00m\n\u001b[1;32m    102\u001b[0m     \u001b[38;5;66;03m# valid\u001b[39;00m\n\u001b[1;32m    103\u001b[0m     valid_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_valid_epoch(epoch_idx, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalid_dataloader) \u001b[38;5;66;03m# mean loss of this epoch\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[33], line 191\u001b[0m, in \u001b[0;36mTrainer._train_epoch\u001b[0;34m(self, epoch_idx, train_dataloader)\u001b[0m\n\u001b[1;32m    189\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(batch) \u001b[38;5;66;03m# tensor(bs, 1)\u001b[39;00m\n\u001b[1;32m    190\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mcalculate_loss(output, label) \u001b[38;5;66;03m# output: (bs, 1), label: (bs, 1)\u001b[39;00m\n\u001b[0;32m--> 191\u001b[0m pbar\u001b[38;5;241m.\u001b[39mset_postfix(loss\u001b[38;5;241m=\u001b[39mloss\u001b[38;5;241m.\u001b[39mitem())\n\u001b[1;32m    192\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_nan(loss)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=256, shuffle=False,num_workers=0, collate_fn=collate_fn)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=256, shuffle=False,num_workers=0, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=256, shuffle=False,num_workers=0, collate_fn=collate_fn)\n",
    "\n",
    "class BGE_FTModel(torch.nn.Module):\n",
    "    def __init__(self, rag_model):\n",
    "        super(BGE_FTModel, self).__init__()\n",
    "        logging.info(f\"Initializing Model Based on path: {rag_model}\")\n",
    "        self.jd_retriever = AutoModel.from_pretrained(rag_model).to(device)\n",
    "        self.cv_retriever = AutoModel.from_pretrained(rag_model).to(device)\n",
    "        self.predictor = nn.Sequential(\n",
    "            nn.Linear(1024, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 1),\n",
    "            nn.Sigmoid()\n",
    "        ).to(device)\n",
    "\n",
    "        self.loss_fn = nn.BCELoss()\n",
    "        # xavier initialization for predictor\n",
    "        for m in self.predictor:\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "        \n",
    "        logging.info(f\"Frozing Parameters...\")\n",
    "        self.frozen_target_parameters()\n",
    "        logging.info(f\"Model Initialized.\")\n",
    "        self.print_trainable_parameters()\n",
    "\n",
    "\n",
    "    def forward(self, sample):\n",
    "        \"\"\"sample: dict like {\n",
    "            \"model_input_jd\": {\"input_ids\": tensor, \"attention_mask\": tensor, \"token_type_ids\": tensor},\n",
    "            \"model_input_cv\": {\"input_ids\": tensor, \"attention_mask\": tensor, \"token_type_ids\": tensor}\n",
    "            \"label\": tensor\n",
    "        }\n",
    "        \"\"\"\n",
    "        jd = {k: v.squeeze(1) for k, v in sample[\"model_input_jd\"].items()}\n",
    "        # (input_ids: tensor(bs, seq_len), attention_mask: tensor(bs, seq_len), token_type_ids: tensor(bs, seq_len))\n",
    "        cv = {k: v.squeeze(1) for k, v in sample[\"model_input_cv\"].items()}\n",
    "        jd_output = self.jd_retriever(**jd)[0][:, 0]\n",
    "        cv_output = self.cv_retriever(**cv)[0][:, 0] # (bs, seq_len)\n",
    "        # concat jd and cv\n",
    "        concat_output = torch.cat((jd_output, cv_output), 1)\n",
    "        return self.predictor(concat_output)\n",
    "    \n",
    "    def calculate_loss(self, output, label):\n",
    "        #TODO: Apply more innovative loss functions.\n",
    "        return self.loss_fn(output, label)\n",
    "\n",
    "    def frozen_target_parameters(self):\n",
    "        for param in self.jd_retriever.parameters():\n",
    "            param.requires_grad = False\n",
    "        for param in self.cv_retriever.parameters():\n",
    "            param.requires_grad = False\n",
    "    \n",
    "    def print_trainable_parameters(self):\n",
    "        total_params = sum(p.numel() for p in self.parameters())\n",
    "        trainable_params = sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "        logging.info(f\"Trainable Params: {trainable_params}. Total Params: {total_params}. Trainable Paramaters Ratio: {trainable_params/total_params}\")\n",
    "\n",
    "class Trainer(object):\n",
    "    def __init__(self, model, train_dataloader, valid_dataloader, test_dataloader, optimizer, eval_step, verbose=True):\n",
    "        self.model = model\n",
    "        self.train_dataloader = train_dataloader\n",
    "        self.valid_dataloader = valid_dataloader\n",
    "        self.test_dataloader = test_dataloader\n",
    "        self.clip_grad_norm = None\n",
    "        self.optimizer = optimizer\n",
    "        self.eval_step = 2\n",
    "\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def train(self, epochs, early_stopping_epochs=10):\n",
    "        train_loss = valid_loss = float('inf')\n",
    "        \n",
    "        # the below init values are for early stopping\n",
    "        best_valid = cur_best_valid = float('inf')\n",
    "        cur_step_from_best_val = 0\n",
    "        \n",
    "        for epoch_idx in range(epochs):\n",
    "            # train\n",
    "            train_loss = self._train_epoch(epoch_idx, self.train_dataloader) # mean loss of this epoch\n",
    "\n",
    "            # valid\n",
    "            valid_loss = self._valid_epoch(epoch_idx, self.valid_dataloader) # mean loss of this epoch\n",
    "            if self.verbose:\n",
    "                logging.info(f\"Epoch {epoch_idx} Train mean Loss: {train_loss:.4f}, Valid mean Loss: {valid_loss:.4f}\")\n",
    "                \n",
    "            # early stopping\n",
    "            if (epoch_idx + 1) % self.eval_step == 0:\n",
    "                if self.verbose:\n",
    "                    logging.info(f\"Epoch {epoch_idx + 1} starts early stopping check.\")\n",
    "                cur_best_valid, cur_step_from_best_val, stop_flag, update_flag = self._early_stopping(\n",
    "                    valid_loss, cur_best_valid, cur_step_from_best_val, early_stopping_epochs)\n",
    "                \n",
    "                if update_flag:\n",
    "                    best_valid = cur_best_valid\n",
    "            \n",
    "                if stop_flag:\n",
    "                    if self.verbose:\n",
    "                        logging.info(f\"Early stopping at epoch {epoch_idx}\")\n",
    "                    break\n",
    "        \n",
    "        return best_valid\n",
    "\n",
    "    \n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def eval(self, evaluator):\n",
    "        \"\"\"\n",
    "        Using the test dataloader to evaluate the model.\n",
    "        For each (cv, jd) pair, we predict the probability of the cv being browsed.\n",
    "        The evaluation results are saved to the save_path as the following format:\n",
    "\n",
    "        The evaluation matriceare all based on top-k selection. for each cv_i, the \n",
    "        top-k are selected from all (cv_i, jd) pairs that appear in the test set. \n",
    "        After consideration, due to the context of precise-recommendation matching \n",
    "        task, we decide if jd_j are in the testset records but not being recorded \n",
    "        with cv_i in the testset, we will not consider jd_j in the top-k selection\n",
    "        for cv_i.\n",
    "        \n",
    "        params:\n",
    "            evaluator: Evaluator, the evaluator for the model.\n",
    "\n",
    "        return:\n",
    "            result: list\n",
    "        \"\"\"\n",
    "        # set model to eval mode\n",
    "        if self.verbose:\n",
    "            logging.info(\"Start evaluating on test set\")\n",
    "        self.model.eval()\n",
    "\n",
    "\n",
    "        pbar = tqdm(enumerate(self.test_dataloader), total=len(self.test_dataloader), desc=\"Matrices Evaluation     \")\n",
    "       \n",
    "        # predicting scores, while saving the predictions records.\n",
    "        for step, batch in pbar:\n",
    "            uid = batch[\"user_id\"] # List of length bs\n",
    "            batch_inputs = dict2device(batch[\"model_input\"], device) # {\"model_input_jd\": dict, \"model_input_cv\": dict, \"label\": tensor}\n",
    "            scores = self.model(batch_inputs).squeeze(-1).cpu().tolist()\n",
    "            labels = batch_inputs[\"label\"].squeeze(-1).cpu().tolist()\n",
    "            evaluator.collect(uid, scores, labels)\n",
    "\n",
    "        # evaluate the results\n",
    "        results, results_str = evaluator.evaluate([1, 5, 10])\n",
    "        return results, results_str\n",
    "\n",
    "    # below is indirect copy from https://github.com/hyp1231/SHPJF/tree/master/model\n",
    "    def _train_epoch(self, epoch_idx: int, train_dataloader: DataLoader):\n",
    "        \"\"\"Train the model in an epoch\n",
    "\n",
    "        Args:\n",
    "            epoch_idx (int): The current epoch id.\n",
    "            train_data (DataLoader): The train data.\n",
    "\n",
    "        Returns:\n",
    "            float/tuple: The sum of loss returned by all batches in this epoch. If the loss in each batch contains\n",
    "            multiple parts and the model return these multiple parts loss instead of the sum of loss, it will return a\n",
    "            tuple which includes the sum of loss in each part.\n",
    "        \"\"\"\n",
    "        self.model.train()\n",
    "        total_loss = 0\n",
    "        total_batches = len(train_dataloader) \n",
    "    \n",
    "        pbar = tqdm(enumerate(train_dataloader), total=total_batches, desc=f\"Epoch {epoch_idx} Train\")\n",
    "\n",
    "        for step, batch in pbar: # batch: {\"user_id\": tensor, \"job_id\": tensor, \"model_input\": dict}\n",
    "            batch = dict2device(batch[\"model_input\"], device) # batch: {\"model_input_jd\": dict, \"model_input_cv\": dict, \"label\": tensor}\n",
    "            label = batch[\"label\"].unsqueeze(1).to(device) # (bs, 1)\n",
    "            self.optimizer.zero_grad()\n",
    "            output = self.model(batch) # tensor(bs, 1)\n",
    "            loss = self.model.calculate_loss(output, label) # output: (bs, 1), label: (bs, 1)\n",
    "            pbar.set_postfix(loss=loss.item())\n",
    "            total_loss += loss.item()\n",
    "            self._check_nan(loss)\n",
    "\n",
    "            loss.backward()\n",
    "            if self.clip_grad_norm:\n",
    "                clip_grad_norm_(self.model.parameters(), **self.clip_grad_norm)\n",
    "            self.optimizer.step()\n",
    "    \n",
    "        return total_loss / total_batches\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def _valid_epoch(self, epoch_idx: int, valid_dataloader: DataLoader):\n",
    "        \"\"\"valid the model with valid data by calculate the loss\n",
    "        \"\"\"\n",
    "        \n",
    "        # set model to eval mode\n",
    "        self.model.eval()\n",
    "        total_loss = 0\n",
    "        total_batches = len(valid_dataloader)\n",
    "        pbar = tqdm(enumerate(valid_dataloader), total=total_batches, desc=f\"Epoch {epoch_idx} Valid\")\n",
    "\n",
    "        # calculate loss on validation set\n",
    "        for step, batch in pbar:\n",
    "            batch = dict2device(batch[\"model_input\"], device) # batch: {\"model_input_jd\": dict, \"model_input_cv\": dict, \"label\": tensor}\n",
    "            label = batch[\"label\"].unsqueeze(1) # (bs, 1)\n",
    "            output = self.model(batch) # (bs, 1)\n",
    "            loss = self.model.calculate_loss(output, label) # output: (bs, 1), label: (bs, 1)\n",
    "            pbar.set_postfix(loss=loss.item())\n",
    "            total_loss += loss.item()\n",
    "            self._check_nan(loss)\n",
    "\n",
    "        return total_loss / total_batches\n",
    "    \n",
    "    def _early_stopping(self, value, best, cur_step, max_step):\n",
    "        \"\"\"validation-based early stopping\n",
    "\n",
    "        Args:\n",
    "            value (float): current result\n",
    "            best (float): best result\n",
    "            cur_step (int): the number of consecutive steps that did not exceed the best result\n",
    "            max_step (int): threshold steps for stopping\n",
    "\n",
    "        Returns:\n",
    "            tuple:\n",
    "            - best: float,\n",
    "            best result after this step\n",
    "            - cur_step: int,\n",
    "            the number of consecutive steps that did not exceed the best result after this step\n",
    "            - stop_flag: bool,\n",
    "            whether to stop\n",
    "            - update_flag: bool,\n",
    "            whether to update\n",
    "        \"\"\"\n",
    "        stop_flag = False\n",
    "        update_flag = False\n",
    "        if value > best:\n",
    "            cur_step = 0\n",
    "            best = value\n",
    "            update_flag = True\n",
    "        else:\n",
    "            cur_step += 1\n",
    "            if cur_step > max_step:\n",
    "                stop_flag = True\n",
    "        return best, cur_step, stop_flag, update_flag\n",
    "\n",
    "    def _check_nan(self, loss):\n",
    "        if torch.isnan(loss).any():\n",
    "            raise ValueError(\"Model diverged with loss = NaN\")\n",
    "        return\n",
    "\n",
    "    \n",
    "model = BGE_FTModel(bge_path).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "loss_fn = nn.BCELoss()\n",
    "trainer = Trainer(model, train_loader, valid_loader, test_loader, optimizer, eval_step=1)\n",
    "best_valid = trainer.train(epochs = 1000)\n",
    "evaluator = Evaluator()\n",
    "result, result_str = trainer.eval(evaluator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-07 11:21:07,198 - INFO - Dataset train Loaded. Shape: (511515, 7)\n",
      "2025-01-07 11:21:08,606 - INFO - Dataset valid Loaded. Shape: (63950, 7)\n",
      "2025-01-07 11:21:09,827 - INFO - Dataset test Loaded. Shape: (63941, 7)\n",
      "Epoch 0 Train:   0%|          | 0/511515 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[101, 702, 782,  ...,   0,   0,   0],\n",
      "        [101, 702, 782,  ...,   0,   0,   0],\n",
      "        [101, 702, 782,  ...,   0,   0,   0],\n",
      "        ...,\n",
      "        [101, 702, 782,  ...,   0,   0,   0],\n",
      "        [101, 702, 782,  ...,   0,   0,   0],\n",
      "        [101, 702, 782,  ...,   0,   0,   0]])\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]])\n",
      "tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]])\n",
      "tensor([[ 101, 2266,  855,  ...,    0,    0,    0],\n",
      "        [ 101, 5466,  855,  ...,    0,    0,    0],\n",
      "        [ 101, 2266,  855,  ...,    0,    0,    0],\n",
      "        ...,\n",
      "        [ 101, 4886, 1164,  ...,    0,    0,    0],\n",
      "        [ 101, 2266,  855,  ...,    0,    0,    0],\n",
      "        [ 101, 2266,  855,  ...,    0,    0,    0]])\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]])\n",
      "tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "tensors used as indices must be long, int, byte or bool tensors",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[56], line 21\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m kkk \u001b[38;5;129;01min\u001b[39;00m sample[k][kk]: \u001b[38;5;66;03m# kkk: input_ids, attention_masks, labels\u001b[39;00m\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m kkk \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 21\u001b[0m         \u001b[38;5;28mprint\u001b[39m(sample[k][kk][kkk])\n\u001b[1;32m     22\u001b[0m         sample[k][kk][kkk] \u001b[38;5;241m=\u001b[39m sample[k][kk][kkk]\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mIndexError\u001b[0m: tensors used as indices must be long, int, byte or bool tensors"
     ]
    }
   ],
   "source": [
    "train_dataset = BGE_FTDataset('train', 'dataset/processed_train.csv', tokenizer,ratio=1)\n",
    "valid_dataset = BGE_FTDataset('valid', \"dataset/processed_valid.csv\", tokenizer,ratio=1)\n",
    "test_dataset = BGE_FTDataset('test', \"dataset/processed_test.csv\", tokenizer, ratio=1)\n",
    "\n",
    "total_batches_train = len(train_dataset) \n",
    "total_batches_valid = len(valid_dataset)\n",
    "total_batches_test = len(test_dataset)\n",
    "    \n",
    "pbar = tqdm(enumerate(train_loader), total=len(train_dataset), desc=f\"Epoch {0} Train\")\n",
    "\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-07 11:26:11,122 - INFO - Dataset train Loaded. Shape: (511515, 7)\n",
      "2025-01-07 11:26:12,245 - INFO - Dataset valid Loaded. Shape: (63950, 7)\n",
      "2025-01-07 11:26:13,413 - INFO - Dataset test Loaded. Shape: (63941, 7)\n",
      "2025-01-07 11:26:21,734 - INFO - Dataset train Loaded. Shape: (511515, 7)\n",
      "2025-01-07 11:26:23,107 - INFO - Dataset valid Loaded. Shape: (63950, 7)\n",
      "2025-01-07 11:26:24,287 - INFO - Dataset test Loaded. Shape: (63941, 7)\n",
      "Saving tokenized dataset to JSON: 100%|██████████| 511515/511515 [24:50<00:00, 343.25it/s] \n",
      "Saving tokenized dataset to JSON: 100%|██████████| 63950/63950 [03:06<00:00, 343.56it/s]\n",
      "Saving tokenized dataset to JSON: 100%|██████████| 63941/63941 [03:11<00:00, 334.63it/s]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "train_dataset = BGE_FTDataset('train', 'dataset/processed_train.csv', tokenizer,ratio=1)\n",
    "valid_dataset = BGE_FTDataset('valid', \"dataset/processed_valid.csv\", tokenizer,ratio=1)\n",
    "test_dataset = BGE_FTDataset('test', \"dataset/processed_test.csv\", tokenizer, ratio=1)\n",
    "\n",
    "def save_tokenized_dataset_to_json(dataset, output_file):\n",
    "    \"\"\"\n",
    "    将 IterableDataset 的 tokenized 文本逐条存储到 JSON 文件中。\n",
    "\n",
    "    Args:\n",
    "        dataset (IterableDataset): 实现了 __iter__ 方法的 dataset，如 BGE_FTDataset。\n",
    "        output_file (str): 要保存的 JSON 文件路径。\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        for sample in tqdm(dataset, desc=\"Saving tokenized dataset to JSON\"):\n",
    "            # 将 PyTorch 张量转换为可序列化的格式\n",
    "            serialized_sample = {\n",
    "                \"user_id\": sample[\"user_id\"],  # 字符串或基本数据类型，直接存储\n",
    "                \"job_id\": sample[\"job_id\"],    # 字符串或基本数据类型，直接存储\n",
    "                \"model_input\": {\n",
    "                    \"model_input_cv\": {\n",
    "                        k: v.tolist() for k, v in sample[\"model_input\"][\"model_input_cv\"].items()\n",
    "                    },\n",
    "                    \"model_input_jd\": {\n",
    "                        k: v.tolist() for k, v in sample[\"model_input\"][\"model_input_jd\"].items()\n",
    "                    },\n",
    "                    \"label\": sample[\"model_input\"][\"label\"].item()  # 转为 float\n",
    "                }\n",
    "            }\n",
    "            # 将该样本写入 JSON 文件，每行一个样本\n",
    "            f.write(json.dumps(serialized_sample) + '\\n')\n",
    "\n",
    "train_dataset = BGE_FTDataset('train', 'dataset/processed_train.csv', tokenizer,ratio=1)\n",
    "valid_dataset = BGE_FTDataset('valid', \"dataset/processed_valid.csv\", tokenizer,ratio=1)\n",
    "test_dataset = BGE_FTDataset('test', \"dataset/processed_test.csv\", tokenizer, ratio=1)\n",
    "\n",
    "save_tokenized_dataset_to_json(train_dataset, '/media/wuyuhuan/tokenids/train_token_jsons.json')\n",
    "save_tokenized_dataset_to_json(valid_dataset, '/media/wuyuhuan/tokenids/valid_token_jsons.json')\n",
    "save_tokenized_dataset_to_json(test_dataset, '/media/wuyuhuan/tokenids/test_token_jsons.json')\n",
    "# 示例用法\n",
    "# 假设你已经定义了 BGE_FTDataset\n",
    "# dataset = BGE_FTDataset(mode='train', in_file='path/to/input.csv', tokenizer=your_tokenizer)\n",
    "# save_tokenized_dataset_to_json(dataset, output_file='output.json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model_input_cv': {'input_ids': tensor([[  101,   702,   782,  5042,  1325,   131,   523,  2339,   868,  5307,\n",
       "           1325,   524,  4415,  6608,   170,  3177,  2339,  5052,  4415,   170,\n",
       "          10399,   170,  2825,  3318,   170,  3177,  2339,   170,  3332,  3160,\n",
       "            170,  8645,   170,  6369,  5050,   170,  3440,  3428,   170,  1545,\n",
       "           1400,   170,  8628,   170,  2339,  4923,  3844,  7030,   170,  3765,\n",
       "           6858,  5543,  1213,   170,  5356,  6782,   170,  3302,  1218,   170,\n",
       "           5307,  4415,   170,  2110,   739,  5543,  1213,   170,  1400,  3309,\n",
       "            170,  3302,  1218,  6121,   689,   170,  2145,  2787,   523,  3309,\n",
       "           3307,  6121,   689,   524,   131,  2791,  1765,   772,   120,  2456,\n",
       "           5029,   120,  2456,  3332,   120,  2339,  4923,   523,  2496,  1184,\n",
       "           6121,   689,   524,   924,  7372,   102,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0]])},\n",
       " 'model_input_jd': {'input_ids': tensor([[ 101, 3315, 1062, 1385, 3221,  671, 2157, 1920, 1798,  677, 2356,  821,\n",
       "           689, 8024, 4310, 6499, 3221,  677, 2356, 1062, 1385, 8024, 1062, 1385,\n",
       "          3300, 4708, 2487, 1920, 4638, 1079, 1249,  100, 6956, 7339,  100,  711,\n",
       "          1447, 2339, 2990,  897, 5653, 6844, 4638, 1215, 1062, 4384, 1862, 8024,\n",
       "          2400,  684,  775, 1358,  758, 7372,  671, 7032, 5023, 1744, 2157, 4886,\n",
       "          1164, 8024, 1447, 2339, 3680, 2399, 1744, 1079, 3952, 1469, 1744, 1912,\n",
       "          3952, 8024, 1762, 2145, 3302, 4638, 2339,  868,  704, 3187, 7444, 2857,\n",
       "          2552, 2825, 5543,  680, 6413, 3318, 8024, 1062, 1385,  671, 2190,  671,\n",
       "          1824, 6378, 8024, 6375,  872, 3766, 3300, 1400, 7560,  722, 2569, 8024,\n",
       "          6821, 7027, 3221, 2644, 1355, 2245, 4638, 6629, 4157,  511, 1062, 1385,\n",
       "          1762, 6821, 7027, 3309, 2521, 2644, 4638, 1217, 1057,  102,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0]])},\n",
       " 'label': 1.0}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{'auc': 0.49484767025089593,\n",
    " 'logloss': 0.8752708701473016,\n",
    " 'ndcg@1': 0.20637408568443052,\n",
    " 'precision@1': 0.20637408568443052,\n",
    " 'recall@1': 0.6475362318840581,\n",
    " 'map@1': 0.20637408568443052,\n",
    " 'mrr@1': 0.24705304274269782}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "jd will look like this after decoding: \n",
    "\n",
    "[CLS] 岗 位 职 责 ： 负 责 公 司 各 类 图 书 的 选 题 、 组 稿 、 编 辑 、 审 读 工 作 。 任 职 要 求 ： 1 、 正 规 院 校 一 类 本 科 及 以 上 学 历 ， 英 语 及 相 关 专 业 毕 业 ， 有 专 业 八 级 证 书 ； 2 、 综 合 素 质 佳 ， 表 达 流 畅 ； 3 、 有 较 强 的 责 任 心 及 较 好 的 抗 压 能 力 ； 4 、 心 态 好 ， 繁 忙 工 作 中 依 然 能 尽 职 尽 责 的 完 成 自 己 的 工 作 任 务 ； 5 、 有 初 、 高 中 家 教 工 作 经 验 或 教 师 工 作 经 验 者 尤 佳 ； 6 、 我 们 需 要 的 是 热 爱 教 育 及 出 版 行 业 的 有 志 之 士 ， 认 真 踏 实 ， 求 真 务 实 ， 与 天 星 共 荣 辱 ， 同 创 辉 煌 ！ 福 利 待 遇 ： 1. 六 险 一 金 齐 全 （ 养 老 、 医 疗 、 失 业 、 工 伤 、 生 育 、 住 房 公 积 金 ） ； 2. 五 天 工 作 制 ， 享 受 国 家 法 定 节 假 日 及 各 项 过 节 福 利 ； 3. 丰 厚 的 季 度 奖 和 年 终 奖 ， 并 为 员 工 提 供 每 年 健 康 体 检 ； 4. 带 薪 假 期 （ 年 假 、 婚 假 、 病 假 、 产 假 等 ） ； 5. 每 年 为 员 工 订 阅 上 万 本 各 类 期 刊 杂 志 和 图 书 ， 提 供 再 教 育 补 贴 ； 6. 旅 游 ， 聚 餐 及 各 类 综 艺 活 动 ； 7. 提 供 各 个 岗 位 的 内 部 及 外 派 带 薪 培 训 。 [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
    "\n",
    "cv will look like this after decoding: \n",
    "\n",
    "[CLS] 个 人 简 历 : 期 望 行 业 : 教 育 / 培 训 / 院 校, 贸 易 / 进 出 口, 旅 游 / 度 假 教 育 / 培 训, 旅 游 / 度 假 / 出 入 境 服 务, 贸 易 跟 单 当 前 行 业 : 旅 游 / 度 假 旅 游 / 度 假 / 出 入 境 服 务 工 作 经 历 : 互 联 网 参 考 模 型 osi 七 层 | 外 国 语 言 文 学 | 预 定 | 员 工 | 处 理 | 英 语 教 育 | office | 英 语 | 财 务 部 门 | 商 务 | 酒 店 | 订 票 | 邮 件 | 协 调 | 图 书 | 操 作 | 线 上 | 出 票 | 网 站 | 接 听 | 外 国 | 财 务 | 行 程 | 咨 询 | 协 助 | 企 业 | 编 辑 | 团 队 领 导 | 服 务 | 工 具 | 学 习 能 力 | 出 差 | 英 语 读 写 | 维 护 | 通 讯 | 电 话 | 系 统 | 业 务 | 沟 通 能 力 | 国 际 机 票 操 作 | 客 户 | 规 划 | 机 票 | 订 单 | 国 际 | 大 客 户 | 团 队 | 软 件 | 适 应 能 力 [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "recbole",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
