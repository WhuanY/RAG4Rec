{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wuyuhuan/anaconda3/envs/recbole/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence embeddings: tensor([[-0.0141, -0.0318,  0.0410,  ..., -0.0393, -0.0152,  0.0084],\n",
      "        [-0.0252, -0.0326,  0.0487,  ..., -0.0431, -0.0079,  0.0234]])\n"
     ]
    }
   ],
   "source": [
    "!export CUDA_VISIBLE_DEVICES=1,3\n",
    "bge_path = \"/media/wuyuhuan/bge-small-zh\"\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "# Sentences we want sentence embeddings for\n",
    "sentences = [\"样例数据-1\", \"样例数据-2\"]\n",
    "\n",
    "# Load model from HuggingFace Hub\n",
    "tokenizer = AutoTokenizer.from_pretrained(bge_path)\n",
    "model = AutoModel.from_pretrained(bge_path)\n",
    "model.eval()\n",
    "\n",
    "# Tokenize sentences\n",
    "encoded_input = tokenizer(sentences, padding=\"max_length\", truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "# for s2p(short query to long passage) retrieval task, add an instruction to query (not add instruction for passages)\n",
    "# encoded_input = tokenizer([instruction + q for q in queries], padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "# Compute token embeddings\n",
    "with torch.no_grad():\n",
    "    model_output = model(**encoded_input)\n",
    "    # Perform pooling. In this case, cls pooling.\n",
    "    sentence_embeddings = model_output[0][:, 0]\n",
    "# normalize embeddings\n",
    "sentence_embeddings = torch.nn.functional.normalize(sentence_embeddings, p=2, dim=1)\n",
    "print(\"Sentence embeddings:\", sentence_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 512])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_input['input_ids'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading whole training data: df.iterrow() approach\n",
    "```python\n",
    "def tokenize_data(self, load_batch_size=1000):\n",
    "        \"\"\"\n",
    "        Tokenize the data in the dataset. Modify the dataset in place to \n",
    "        reduce memory costs.\n",
    "        \"\"\"\n",
    "        iter_data = tqdm(self.dataset.iterrows(), total=self.dataset.shape[0], desc=f\"Tokenizing {self.mode} Dataset\")\n",
    "        for idx, row in iter_data:\n",
    "            cv = row['cv'] \n",
    "            jd = row['jd']\n",
    "            # TODO: More on Modifying the Tokenizing methods.\n",
    "            tokenized_cv = self.tokenizer(cv, return_tensors='pt', padding='max_length', truncation=True, max_length=512) # (input_ids, attention_mask, token_type_ids): (bs, seq_len)\n",
    "            tokenized_jd = self.tokenizer(jd, return_tensors='pt', padding='max_length', truncation=True, max_length=512) # (input_ids, attention_mask, token_type_ids): (bs, seq_len)\n",
    "            self.data.append((row['user_id:token'], row['job_id:token'], tokenized_cv, tokenized_jd, torch.tensor(row['browsed:label'], dtype=torch.float32)))\n",
    "        return \n",
    "```\n",
    "This approach costs around **20-30** min to load the training dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/recbole/lib/python3.12/site-packages/IPython/core/async_helpers.py:128\u001b[0m, in \u001b[0;36m_pseudo_sync_runner\u001b[0;34m(coro)\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;124;03mA runner that does not really allow async execution, and just advance the coroutine.\u001b[39;00m\n\u001b[1;32m    122\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;124;03mCredit to Nathaniel Smith\u001b[39;00m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 128\u001b[0m     coro\u001b[38;5;241m.\u001b[39msend(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m exc\u001b[38;5;241m.\u001b[39mvalue\n",
      "File \u001b[0;32m~/anaconda3/envs/recbole/lib/python3.12/site-packages/IPython/core/interactiveshell.py:3284\u001b[0m, in \u001b[0;36mInteractiveShell.run_cell_async\u001b[0;34m(self, raw_cell, store_history, silent, shell_futures, transformed_cell, preprocessing_exc_tuple, cell_id)\u001b[0m\n\u001b[1;32m   3282\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m store_history:\n\u001b[1;32m   3283\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhistory_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 3284\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhistory_manager\u001b[38;5;241m.\u001b[39mstore_inputs(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexecution_count, cell, raw_cell)\n\u001b[1;32m   3285\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m silent:\n\u001b[1;32m   3286\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39mlog(cell, raw_cell)\n",
      "File \u001b[0;32m~/anaconda3/envs/recbole/lib/python3.12/site-packages/IPython/core/history.py:792\u001b[0m, in \u001b[0;36mHistoryManager.store_inputs\u001b[0;34m(self, line_num, source, source_raw)\u001b[0m\n\u001b[1;32m    789\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_hist_parsed\u001b[38;5;241m.\u001b[39mappend(source)\n\u001b[1;32m    790\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_hist_raw\u001b[38;5;241m.\u001b[39mappend(source_raw)\n\u001b[0;32m--> 792\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdb_input_cache_lock:\n\u001b[1;32m    793\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdb_input_cache\u001b[38;5;241m.\u001b[39mappend((line_num, source, source_raw))\n\u001b[1;32m    794\u001b[0m     \u001b[38;5;66;03m# Trigger to flush cache and write to DB.\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%time\n",
    "bge_path = \"/media/wuyuhuan/bge-small-zh\"\n",
    "from torch.utils.data import Dataset, IterableDataset\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.clip_grad import clip_grad_norm_\n",
    "\n",
    "import logging\n",
    "\n",
    "import time\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "logging.info(f\"Usable GPU: {torch.cuda.device_count()}\")  \n",
    "tokenizer = AutoTokenizer.from_pretrained(bge_path)\n",
    "\n",
    "def same_seed(seed):\n",
    "    '''Fixes random number generator seeds for reproducibility.'''\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "same_seed(42)\n",
    "\n",
    "class BGE_FTDataset(IterableDataset):\n",
    "    def __init__(self, mode: str, in_file: str, tokenizer: AutoTokenizer, ratio: float = 1.0):\n",
    "        \"\"\"\n",
    "        mode: str, one of ['train', 'valid', 'test']\n",
    "        in_file: str, path to the input csv file.\n",
    "        tokenizer: AutoTokenizer, tokenizer for the model.\n",
    "        ratio: float, the ratio of the data to be used. Default: 1.0. \n",
    "            set to 0.01 for functionality testing.\n",
    "        \"\"\"\n",
    "        self.mode = mode\n",
    "        self.dataset = pd.read_csv(in_file).sample(frac=ratio)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(bge_path)\n",
    "        logging.info(f\"Dataset {mode} Loaded. Shape: {self.dataset.shape}\")\n",
    "        self.data = []\n",
    "        self.tokenize_data() # -> self.data will look like: [(user_id, job_id, tokenized_cv, tokenized_jd, label), ...] \n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.dataset.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"user_id\": self.data[idx][0],\n",
    "            \"job_id\": self.data[idx][1],\n",
    "            \"model_input\": {\n",
    "                \"model_input_cv\": self.data[idx][2],\n",
    "                \"model_input_jd\": self.data[idx][3],\n",
    "                \"label\": self.data[idx][4],\n",
    "            }\n",
    "        }\n",
    "        \n",
    "    def tokenize_data(self):\n",
    "        \"\"\"\n",
    "        Tokenize the data in the dataset. Modify the dataset in place to \n",
    "        reduce memory costs.\n",
    "        \"\"\"\n",
    "        iter_data = tqdm(self.dataset.iterrows(), total=self.dataset.shape[0], desc=f\"Tokenizing {self.mode} Dataset\")\n",
    "        for idx, row in iter_data:\n",
    "            cv = row['cv'] \n",
    "            jd = row['jd']\n",
    "            # TODO: More on Modifying the Tokenizing methods.\n",
    "            tokenized_cv = self.tokenizer(cv, return_tensors='pt', padding='max_length', truncation=True, max_length=512) # (input_ids, attention_mask, token_type_ids): (bs, seq_len)\n",
    "            tokenized_jd = self.tokenizer(jd, return_tensors='pt', padding='max_length', truncation=True, max_length=512) # (input_ids, attention_mask, token_type_ids): (bs, seq_len)\n",
    "            self.data.append((row['user_id:token'], row['job_id:token'], tokenized_cv, tokenized_jd, torch.tensor(row['browsed:label'], dtype=torch.float32)))\n",
    "        return \n",
    "        \n",
    "train_dataset = BGE_FTDataset('train', 'dataset/processed_train.csv', tokenizer,ratio=1)\n",
    "# valid_dataset = BGE_FTDataset('valid', \"dataset/processed_valid.csv\", tokenizer,ratio=1)\n",
    "# test_dataset = BGE_FTDataset('test', \"dataset/processed_test.csv\", tokenizer, ratio=5)\n",
    "\n",
    "# train_dataset[0]\n",
    "# valid_dataset[0]\n",
    "# test_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "batch approach costs around 3 min.\n",
    "but the problem is how to extract\n",
    "```python\n",
    "def batch_tokenize(self):\n",
    "    batch_size = 1000\n",
    "    results = []\n",
    "    \n",
    "    iter_data = tqdm(\n",
    "        range(0, len(self.data), batch_size),\n",
    "        total= len(self.data) // batch_size + 1,\n",
    "        desc=f\"Tokenizing {self.mode} Dataset\"\n",
    "    )\n",
    "    for i in iter_data:\n",
    "        batch = self.data.iloc[i:i+batch_size]\n",
    "        # 批量tokenize\n",
    "        batch_tokens = self.tokenizer(\n",
    "            batch['cv'].tolist(),\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        results.append(batch_tokens)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.tokenization_utils_base.BatchEncoding"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_dataset.data['model_input_cv'].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Theoretical estimation: 5.72 GB\n"
     ]
    }
   ],
   "source": [
    "def estimate_memory_theoretical():\n",
    "    num_samples = 500000\n",
    "    seq_length = 512\n",
    "    \n",
    "    # 计算主要组件大小\n",
    "    input_ids_size = num_samples * seq_length * 4  # int32\n",
    "    attention_mask_size = num_samples * seq_length * 4  # int32\n",
    "    token_type_ids_size = num_samples * seq_length * 4  # int32\n",
    "    \n",
    "    # 总大小（字节）\n",
    "    total_bytes = (input_ids_size + attention_mask_size + token_type_ids_size) * 2 # include cv and jd\n",
    "    \n",
    "    # 转换为更易读的单位\n",
    "    total_gb = total_bytes / (1024**3)\n",
    "    \n",
    "    print(f\"Theoretical estimation: {total_gb:.2f} GB\")\n",
    "\n",
    "estimate_memory_theoretical()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "recbole",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
